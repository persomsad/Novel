"""Novel Agent Tools

å®ç° 5 ä¸ªæ ¸å¿ƒå·¥å…·ï¼š
1. read_file - è¯»å–ä»»æ„æ–‡ä»¶
2. write_chapter - åˆ›å»ºæ–°ç« èŠ‚
3. search_content - æœç´¢å…³é”®è¯
4. verify_strict_timeline - æ—¶é—´çº¿ç²¾ç¡®éªŒè¯
5. verify_strict_references - å¼•ç”¨å®Œæ•´æ€§éªŒè¯
"""

import json
import os
import re
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any

import frontmatter  # type: ignore
from langchain_core.tools import tool as lc_tool

from . import nervus_cli
from .logging_config import get_logger
from .tools_creative import dialogue_enhancer, plot_twist_generator, scene_transition

logger = get_logger(__name__)


def read_file(path: str) -> str:
    """è¯»å–æ–‡ä»¶å†…å®¹

    Args:
        path: æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰

    Returns:
        æ–‡ä»¶å†…å®¹

    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        PermissionError: æ— è¯»å–æƒé™
    """
    file_path = Path(path)
    logger.debug(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {path}")

    if not file_path.exists():
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")

    try:
        content = file_path.read_text(encoding="utf-8")
        logger.info(f"æˆåŠŸè¯»å–æ–‡ä»¶: {path} ({len(content)} å­—ç¬¦)")
        return content
    except PermissionError:
        logger.error(f"æ— æƒé™è¯»å–æ–‡ä»¶: {path}")
        raise
    except Exception as e:
        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {path}, é”™è¯¯: {e}")
        raise


def write_chapter(number: int, content: str, base_dir: str = "chapters") -> str:
    """åˆ›å»ºæ–°ç« èŠ‚

    Args:
        number: ç« èŠ‚ç¼–å·ï¼ˆ1-999ï¼‰
        content: ç« èŠ‚å†…å®¹
        base_dir: ç« èŠ‚ç›®å½•ï¼ˆé»˜è®¤: chaptersï¼‰

    Returns:
        åˆ›å»ºçš„æ–‡ä»¶è·¯å¾„

    Raises:
        ValueError: ç« èŠ‚ç¼–å·æ— æ•ˆ
        OSError: æ–‡ä»¶ç³»ç»Ÿé”™è¯¯
    """
    logger.debug(f"æ­£åœ¨åˆ›å»ºç« èŠ‚: ç¼–å·={number}, ç›®å½•={base_dir}")

    if not 1 <= number <= 999:
        logger.error(f"æ— æ•ˆçš„ç« èŠ‚ç¼–å·: {number}")
        raise ValueError(f"ç« èŠ‚ç¼–å·å¿…é¡»åœ¨ 1-999 ä¹‹é—´ï¼Œå½“å‰: {number}")

    try:
        # åˆ›å»ºç›®å½•
        chapters_dir = Path(base_dir)
        chapters_dir.mkdir(parents=True, exist_ok=True)

        # æ ¼å¼åŒ–æ–‡ä»¶åï¼šch001.md, ch002.md, ...
        filename = f"ch{number:03d}.md"
        file_path = chapters_dir / filename

        # å†™å…¥å†…å®¹
        file_path.write_text(content, encoding="utf-8")

        logger.info(f"æˆåŠŸåˆ›å»ºç« èŠ‚: {file_path} ({len(content)} å­—ç¬¦)")
        return str(file_path)
    except OSError as e:
        logger.error(f"åˆ›å»ºç« èŠ‚å¤±è´¥: {e}")
        raise


def search_content(keyword: str, search_dir: str = ".") -> list[dict[str, str]]:
    """æœç´¢å…³é”®è¯

    ä½¿ç”¨ ripgrep åœ¨æŒ‡å®šç›®å½•æœç´¢å…³é”®è¯

    Args:
        keyword: æœç´¢å…³é”®è¯
        search_dir: æœç´¢ç›®å½•ï¼ˆé»˜è®¤: å½“å‰ç›®å½•ï¼‰

    Returns:
        åŒ¹é…ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«:
        - file: æ–‡ä»¶è·¯å¾„
        - line: è¡Œå·
        - content: åŒ¹é…å†…å®¹
    """
    logger.debug(f"æœç´¢å…³é”®è¯: '{keyword}' åœ¨ç›®å½•: {search_dir}")

    try:
        # ä½¿ç”¨ rg æœç´¢
        result = subprocess.run(
            ["rg", "--json", "--fixed-strings", keyword, search_dir],
            capture_output=True,
            text=True,
            check=False,  # ä¸æŠ›å‡ºå¼‚å¸¸ï¼ˆæ²¡æœ‰åŒ¹é…æ—¶ rg è¿”å› 1ï¼‰
        )

        if result.returncode not in (0, 1):
            # å…¶ä»–é”™è¯¯ï¼ˆ2=æœç´¢é”™è¯¯ï¼‰
            logger.error(f"ripgrepæœç´¢å¤±è´¥: {result.stderr}")
            raise RuntimeError(f"æœç´¢å¤±è´¥: {result.stderr}")

        # è§£æ JSON è¾“å‡º
        import json

        matches: list[dict[str, str]] = []
        for line in result.stdout.strip().split("\n"):
            if not line:
                continue
            data = json.loads(line)
            if data.get("type") == "match":
                match_data = data["data"]
                matches.append(
                    {
                        "file": match_data["path"]["text"],
                        "line": str(match_data["line_number"]),
                        "content": match_data["lines"]["text"].strip(),
                    }
                )

        logger.info(f"æœç´¢å®Œæˆ: æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
        return matches

    except FileNotFoundError:
        # ripgrep æœªå®‰è£…ï¼Œå›é€€åˆ° Python å®ç°
        logger.warning("ripgrepæœªå®‰è£…ï¼Œä½¿ç”¨Python fallbackå®ç°")
        return _search_content_fallback(keyword, search_dir)


def _search_content_fallback(keyword: str, search_dir: str) -> list[dict[str, str]]:
    """æœç´¢å…³é”®è¯ï¼ˆPython å®ç°ä½œä¸ºåå¤‡ï¼‰"""
    matches: list[dict[str, str]] = []
    search_path = Path(search_dir)

    # åªæœç´¢ .md æ–‡ä»¶
    for file_path in search_path.rglob("*.md"):
        try:
            content = file_path.read_text(encoding="utf-8")
            for line_num, line in enumerate(content.splitlines(), 1):
                if keyword in line:
                    matches.append(
                        {
                            "file": str(file_path),
                            "line": str(line_num),
                            "content": line.strip(),
                        }
                    )
        except Exception:
            # è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶
            continue

    return matches


def _load_continuity_index(path: Path | None = None) -> dict[str, Any]:
    target = path or Path("data/continuity/index.json")
    if not target.exists():
        raise FileNotFoundError(
            f"è¿ç»­æ€§ç´¢å¼• {target} ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ `poetry run novel-agent refresh-memory`ã€‚"
        )
    result: dict[str, Any] = json.loads(target.read_text(encoding="utf-8"))
    return result


def _get_nervus_db_path(explicit: str | None = None) -> str | None:
    return explicit or os.getenv("NERVUSDB_DB_PATH")


def _fetch_nervus_events(db_path: str) -> list[tuple[str, str]]:
    try:
        result = nervus_cli.cypher_query(
            db_path,
            """
            MATCH (ch:Chapter)-[:HAS_EVENT]->(e:Event)
            RETURN ch.id as chapter_id, e.timestamp as timestamp
            ORDER BY chapter_id, timestamp
            """,
        )
    except Exception as exc:
        raise RuntimeError(f"NervusDB æ—¶é—´çº¿æŸ¥è¯¢å¤±è´¥: {exc}") from exc

    rows = result.get("rows") if isinstance(result, dict) else result
    if not rows and isinstance(result, dict) and "result" in result:
        rows = result["result"]
    events: list[tuple[str, str]] = []
    for row in rows or []:
        chapter_id = row.get("chapter_id") or row.get("CHAPTER_ID")
        timestamp = row.get("timestamp") or row.get("TIMESTAMP")
        if chapter_id and timestamp:
            events.append((str(chapter_id), str(timestamp)))
    return events


def _fetch_nervus_references(db_path: str) -> set[tuple[str, str]]:
    try:
        result = nervus_cli.cypher_query(
            db_path,
            """
            MATCH (ch:Chapter)-[:USES_REFERENCE]->(r:Reference)
            RETURN ch.id as chapter_id, r.id as ref_id
            """,
        )
    except Exception as exc:
        raise RuntimeError(f"NervusDB å¼•ç”¨æŸ¥è¯¢å¤±è´¥: {exc}") from exc

    rows = result.get("rows") if isinstance(result, dict) else result
    if not rows and isinstance(result, dict) and "result" in result:
        rows = result["result"]
    refs: set[tuple[str, str]] = set()
    for row in rows or []:
        chapter_id = row.get("chapter_id") or row.get("CHAPTER_ID")
        ref_id = row.get("ref_id") or row.get("REF_ID")
        if chapter_id and ref_id:
            refs.add((str(chapter_id), str(ref_id)))
    return refs


def verify_strict_timeline(
    index_path: Path | None = None,
    *,
    db_path: str | None = None,
) -> dict[str, Any]:
    """æ—¶é—´çº¿ç²¾ç¡®éªŒè¯ï¼ˆå¢å¼ºç‰ˆï¼šè¾“å‡ºè¡Œå·å’Œä¿®å¤å»ºè®®ï¼‰

    Returns:
        {
            "errors": [
                {
                    "file": "chapters/ch002.md",
                    "line": 42,
                    "type": "timeline_inconsistency",
                    "message": "æ—¶é—´å€’é€€ï¼šå‰ä¸€ç« æ˜¯ 2077-03-15ï¼Œæ­¤å¤„æ˜¯ 2077-03-10",
                    "suggestion": "å°† [TIME:2077-03-10] æ”¹ä¸º [TIME:2077-03-16] æˆ–æ›´æ™š",
                    "current_value": "2077-03-10",
                    "expected_value": "2077-03-16"
                }
            ],
            "warnings": [...],
            "summary": {
                "total_errors": 2,
                "total_warnings": 1,
                "auto_fixable": True
            }
        }
    """

    data = _load_continuity_index(index_path)
    errors: list[dict[str, Any]] = []
    warnings: list[dict[str, Any]] = []

    def parse_date(value: str) -> datetime | None:
        for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%Y.%m.%d"):
            try:
                return datetime.strptime(value, fmt)
            except ValueError:
                continue
        return None

    last_date: datetime | None = None
    last_chapter_id: str = ""

    for chapter in sorted(data.get("chapters", []), key=lambda c: c.get("chapter_id", "")):
        chapter_id = chapter.get("chapter_id")
        for marker in chapter.get("time_markers", []):
            value = marker.get("value")
            line_number = marker.get("line", 0)
            dt = parse_date(value)

            if not dt:
                warnings.append(
                    {
                        "file": f"chapters/{chapter_id}.md",
                        "line": line_number,
                        "type": "unparseable_time",
                        "message": f"æ—¶é—´æ ‡è®° `{value}` æ— æ³•è§£æ",
                        "suggestion": "è¯·ä½¿ç”¨ YYYY-MM-DDã€YYYY/MM/DD æˆ– YYYY.MM.DD æ ¼å¼",
                        "current_value": value,
                    }
                )
                continue

            if last_date and dt < last_date:
                # è®¡ç®—å»ºè®®çš„æ—¥æœŸï¼ˆå‰ä¸€ä¸ªæ—¥æœŸ + 1å¤©ï¼‰
                from datetime import timedelta

                suggested_date = (last_date + timedelta(days=1)).strftime("%Y-%m-%d")

                prev_date = last_date.date()
                curr_date = dt.date()
                errors.append(
                    {
                        "file": f"chapters/{chapter_id}.md",
                        "line": line_number,
                        "type": "timeline_inconsistency",
                        "message": (
                            f"æ—¶é—´å€’é€€ï¼šå‰ä¸€ç« èŠ‚ ({last_chapter_id}) "
                            f"æ˜¯ {prev_date}ï¼Œæ­¤å¤„æ˜¯ {curr_date}"
                        ),
                        "suggestion": f"å°† [TIME:{value}] æ”¹ä¸º [TIME:{suggested_date}] æˆ–æ›´æ™š",
                        "current_value": value,
                        "expected_value": suggested_date,
                        "previous_chapter": last_chapter_id,
                        "previous_date": str(prev_date),
                    }
                )

            last_date = dt
            last_chapter_id = chapter_id

    # NervusDB æ¯”å¯¹
    nervus_db = _get_nervus_db_path(db_path)
    if nervus_db:
        try:
            db_events = _fetch_nervus_events(nervus_db)
            db_set = set(db_events)
            local_set = {
                (chapter.get("chapter_id"), marker.get("value"))
                for chapter in data.get("chapters", [])
                for marker in chapter.get("time_markers", [])
            }

            for chapter in data.get("chapters", []):
                cid = chapter.get("chapter_id")
                for marker in chapter.get("time_markers", []):
                    value = marker.get("value")
                    line_number = marker.get("line", 0)
                    if (cid, value) not in db_set:
                        errors.append(
                            {
                                "file": f"chapters/{cid}.md",
                                "line": line_number,
                                "type": "missing_in_nervus",
                                "message": f"æ—¶é—´æ ‡è®° `{value}` æœªå†™å…¥ NervusDB",
                                "suggestion": "è¿è¡Œ 'novel-agent memory ingest' åŒæ­¥åˆ° NervusDB",
                                "current_value": value,
                            }
                        )

            extra = db_set - local_set
            for cid, value in sorted(extra):
                warnings.append(
                    {
                        "file": f"chapters/{cid}.md",
                        "line": 0,
                        "type": "extra_in_nervus",
                        "message": f"NervusDB ä¸­å­˜åœ¨æœªåœ¨ç« èŠ‚å‡ºç°çš„æ—¶é—´ `{value}`",
                        "suggestion": f"æ£€æŸ¥ç« èŠ‚ {cid} æ˜¯å¦åˆ é™¤äº†æ­¤æ—¶é—´æ ‡è®°",
                        "current_value": value,
                    }
                )
        except RuntimeError as exc:
            warnings.append(
                {
                    "file": "NervusDB",
                    "line": 0,
                    "type": "nervus_error",
                    "message": str(exc),
                    "suggestion": "æ£€æŸ¥ NervusDB é…ç½®å’Œæ•°æ®åº“è¿æ¥",
                }
            )

    # è®¡ç®—è‡ªåŠ¨ä¿®å¤èƒ½åŠ›
    auto_fixable = all(err.get("expected_value") for err in errors)

    return {
        "errors": errors,
        "warnings": warnings,
        "summary": {
            "total_errors": len(errors),
            "total_warnings": len(warnings),
            "auto_fixable": auto_fixable,
        },
    }


def verify_strict_references(
    index_path: Path | None = None,
    *,
    db_path: str | None = None,
) -> dict[str, Any]:
    """å¼•ç”¨å®Œæ•´æ€§éªŒè¯ï¼ˆå¢å¼ºç‰ˆï¼šè¾“å‡ºè¡Œå·å’Œä¿®å¤å»ºè®®ï¼‰

    Returns:
        {
            "errors": [
                {
                    "file": "chapters/ch003.md",
                    "line": 56,
                    "type": "undefined_reference",
                    "message": "å¼•ç”¨ `sword_of_destiny` æœªå®šä¹‰",
                    "suggestion": "åœ¨ spec/knowledge/ ä¸­æ·»åŠ æ­¤å¼•ç”¨çš„å®šä¹‰ï¼Œæˆ–æ£€æŸ¥æ‹¼å†™é”™è¯¯",
                    "current_value": "sword_of_destiny"
                }
            ],
            "warnings": [...],
            "summary": {
                "total_errors": 1,
                "total_warnings": 2,
                "auto_fixable": False
            }
        }
    """

    data = _load_continuity_index(index_path)
    errors: list[dict[str, Any]] = []
    warnings: list[dict[str, Any]] = []

    reference_map = {ref["id"]: ref["occurrences"] for ref in data.get("references", [])}

    # æ£€æŸ¥æœªä½¿ç”¨çš„å¼•ç”¨å®šä¹‰
    for ref_id, occurrences in reference_map.items():
        if not occurrences:
            warnings.append(
                {
                    "file": "spec/knowledge/",
                    "line": 0,
                    "type": "unused_reference",
                    "message": f"å¼•ç”¨ `{ref_id}` æ— ä»»ä½•ç« èŠ‚ä½¿ç”¨",
                    "suggestion": f"è€ƒè™‘åˆ é™¤æ­¤å¼•ç”¨å®šä¹‰ï¼Œæˆ–åœ¨ç« èŠ‚ä¸­æ·»åŠ  [REF:{ref_id}]",
                    "current_value": ref_id,
                }
            )

    # æ£€æŸ¥æœªå®šä¹‰çš„å¼•ç”¨
    defined = set(reference_map.keys())
    for chapter in data.get("chapters", []):
        chapter_id = chapter.get("chapter_id")
        for ref in chapter.get("references", []):
            ref_id = ref.get("id")
            line_number = ref.get("line", 0)

            if ref_id not in defined:
                errors.append(
                    {
                        "file": f"chapters/{chapter_id}.md",
                        "line": line_number,
                        "type": "undefined_reference",
                        "message": f"å¼•ç”¨ `{ref_id}` æœªå®šä¹‰",
                        "suggestion": "åœ¨ spec/knowledge/ ä¸­æ·»åŠ æ­¤å¼•ç”¨çš„å®šä¹‰ï¼Œæˆ–æ£€æŸ¥æ‹¼å†™é”™è¯¯",
                        "current_value": ref_id,
                    }
                )

    # NervusDB æ¯”å¯¹
    nervus_db = _get_nervus_db_path(db_path)
    if nervus_db:
        try:
            db_refs = _fetch_nervus_references(nervus_db)
            db_set = set(db_refs)
            local_refs = {
                (chapter.get("chapter_id"), ref.get("id"))
                for chapter in data.get("chapters", [])
                for ref in chapter.get("references", [])
            }

            for chapter in data.get("chapters", []):
                cid = chapter.get("chapter_id")
                for ref in chapter.get("references", []):
                    ref_id = ref.get("id")
                    line_number = ref.get("line", 0)

                    if (cid, ref_id) not in db_set:
                        errors.append(
                            {
                                "file": f"chapters/{cid}.md",
                                "line": line_number,
                                "type": "missing_in_nervus",
                                "message": f"å¼•ç”¨ `{ref_id}` æœªå†™å…¥ NervusDB",
                                "suggestion": "è¿è¡Œ 'novel-agent memory ingest' åŒæ­¥åˆ° NervusDB",
                                "current_value": ref_id,
                            }
                        )

            extra_refs = db_set - local_refs
            for cid, ref_id in sorted(extra_refs):
                warnings.append(
                    {
                        "file": f"chapters/{cid}.md",
                        "line": 0,
                        "type": "extra_in_nervus",
                        "message": f"NervusDB ä¸­å­˜åœ¨æœªåœ¨ç« èŠ‚ä½¿ç”¨çš„å¼•ç”¨ `{ref_id}`",
                        "suggestion": f"æ£€æŸ¥ç« èŠ‚ {cid} æ˜¯å¦åˆ é™¤äº†æ­¤å¼•ç”¨",
                        "current_value": ref_id,
                    }
                )
        except RuntimeError as exc:
            warnings.append(
                {
                    "file": "NervusDB",
                    "line": 0,
                    "type": "nervus_error",
                    "message": str(exc),
                    "suggestion": "æ£€æŸ¥ NervusDB é…ç½®å’Œæ•°æ®åº“è¿æ¥",
                }
            )

    # å¼•ç”¨éªŒè¯é€šå¸¸ä¸èƒ½è‡ªåŠ¨ä¿®å¤ï¼ˆéœ€è¦æ‰‹åŠ¨å®šä¹‰æˆ–åˆ é™¤ï¼‰
    auto_fixable = False

    return {
        "errors": errors,
        "warnings": warnings,
        "summary": {
            "total_errors": len(errors),
            "total_warnings": len(warnings),
            "auto_fixable": auto_fixable,
        },
    }


def edit_chapter_lines(
    chapter_number: int,
    start_line: int,
    end_line: int,
    new_content: str,
    base_dir: str = "chapters",
) -> str:
    """ç²¾å‡†ä¿®æ”¹ç« èŠ‚çš„æŒ‡å®šè¡Œ

    Args:
        chapter_number: ç« èŠ‚ç¼–å·
        start_line: èµ·å§‹è¡Œå·ï¼ˆä»1å¼€å§‹ï¼‰
        end_line: ç»“æŸè¡Œå·ï¼ˆåŒ…å«ï¼Œä»1å¼€å§‹ï¼‰
        new_content: æ–°å†…å®¹ï¼ˆæ›¿æ¢æŒ‡å®šè¡Œï¼‰
        base_dir: ç« èŠ‚ç›®å½•

    Returns:
        æ“ä½œç»“æœæè¿°

    Raises:
        FileNotFoundError: ç« èŠ‚æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: è¡Œå·å‚æ•°æ— æ•ˆ

    Example:
        >>> # ä¿®æ”¹ç¬¬10-12è¡Œ
        >>> edit_chapter_lines(1, 10, 12, "æ–°çš„å†…å®¹\\næ›¿æ¢è¿™ä¸‰è¡Œ")
    """
    if chapter_number < 1:
        raise ValueError(f"ç« èŠ‚ç¼–å·å¿…é¡» >= 1ï¼Œå½“å‰: {chapter_number}")

    if start_line < 1 or end_line < start_line:
        raise ValueError(f"è¡Œå·æ— æ•ˆ: start={start_line}, end={end_line}")

    chapter_path = Path(base_dir) / f"ch{chapter_number:03d}.md"

    if not chapter_path.exists():
        raise FileNotFoundError(f"ç« èŠ‚ä¸å­˜åœ¨: {chapter_path}")

    logger.info(f"æ­£åœ¨ä¿®æ”¹ç« èŠ‚ {chapter_number} çš„ç¬¬ {start_line}-{end_line} è¡Œ: {chapter_path}")

    # è¯»å–åŸæ–‡ä»¶
    lines = chapter_path.read_text(encoding="utf-8").splitlines(keepends=True)
    total_lines = len(lines)

    if end_line > total_lines:
        raise ValueError(f"ç»“æŸè¡Œå· {end_line} è¶…å‡ºæ–‡ä»¶æ€»è¡Œæ•° {total_lines}")

    # æ›¿æ¢æŒ‡å®šè¡Œï¼ˆæ³¨æ„ï¼šè¡Œå·ä»1å¼€å§‹ï¼Œæ•°ç»„ç´¢å¼•ä»0å¼€å§‹ï¼‰
    new_lines = new_content.splitlines(keepends=True)

    # ç¡®ä¿æ–°å†…å®¹ä»¥æ¢è¡Œç¬¦ç»“å°¾
    if new_lines and not new_lines[-1].endswith("\n"):
        new_lines[-1] += "\n"

    # ç»„åˆï¼šå‰åŠéƒ¨åˆ† + æ–°å†…å®¹ + ååŠéƒ¨åˆ†
    result_lines = lines[: start_line - 1] + new_lines + lines[end_line:]

    # å†™å›æ–‡ä»¶
    chapter_path.write_text("".join(result_lines), encoding="utf-8")

    logger.info(f"âœ… æˆåŠŸä¿®æ”¹ç« èŠ‚ {chapter_number} (ç¬¬ {start_line}-{end_line} è¡Œ)")
    return (
        f"âœ… æˆåŠŸä¿®æ”¹ç« èŠ‚ {chapter_number} "
        f"(ç¬¬ {start_line}-{end_line} è¡Œï¼Œå…± {len(new_lines)} è¡Œæ–°å†…å®¹)"
    )


def replace_in_file(
    file_path: str,
    search_text: str,
    replacement: str,
    occurrence: int | None = None,
) -> str:
    """åœ¨æ–‡ä»¶ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢æ–‡æœ¬

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        search_text: è¦æŸ¥æ‰¾çš„æ–‡æœ¬
        replacement: æ›¿æ¢æ–‡æœ¬
        occurrence: æ›¿æ¢ç¬¬å‡ æ¬¡å‡ºç°ï¼ˆNone=å…¨éƒ¨æ›¿æ¢ï¼Œ1=ç¬¬ä¸€æ¬¡ï¼Œ2=ç¬¬äºŒæ¬¡...ï¼‰

    Returns:
        æ“ä½œç»“æœæè¿°

    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: search_text ä¸å­˜åœ¨

    Example:
        >>> # æ›¿æ¢æ‰€æœ‰"å¼ ä¸‰"ä¸º"æå››"
        >>> replace_in_file("chapters/ch001.md", "å¼ ä¸‰", "æå››")

        >>> # åªæ›¿æ¢ç¬¬ä¸€æ¬¡å‡ºç°çš„"å¼ ä¸‰"
        >>> replace_in_file("chapters/ch001.md", "å¼ ä¸‰", "æå››", occurrence=1)
    """
    path = Path(file_path)

    if not path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    logger.info(f"æ­£åœ¨æŸ¥æ‰¾æ›¿æ¢: {file_path} ('{search_text}' â†’ '{replacement}')")

    content = path.read_text(encoding="utf-8")

    if search_text not in content:
        raise ValueError(f"æœªæ‰¾åˆ°è¦æ›¿æ¢çš„æ–‡æœ¬: '{search_text}'")

    # è®¡ç®—å‡ºç°æ¬¡æ•°
    count = content.count(search_text)

    if occurrence is not None:
        if occurrence < 1 or occurrence > count:
            raise ValueError(f"occurrence å‚æ•°æ— æ•ˆ: {occurrence} (æ–‡æœ¬å…±å‡ºç° {count} æ¬¡)")

        # æ›¿æ¢æŒ‡å®šçš„ç¬¬ N æ¬¡å‡ºç°
        parts = content.split(search_text)
        new_content = (
            search_text.join(parts[:occurrence])
            + replacement
            + search_text.join(parts[occurrence:])
        )
        replaced_count = 1
    else:
        # æ›¿æ¢æ‰€æœ‰å‡ºç°
        new_content = content.replace(search_text, replacement)
        replaced_count = count

    path.write_text(new_content, encoding="utf-8")

    logger.info(f"âœ… æˆåŠŸæ›¿æ¢ {replaced_count} å¤„æ–‡æœ¬: {file_path}")
    return f"âœ… æˆåŠŸæ›¿æ¢ {replaced_count} å¤„æ–‡æœ¬: {file_path} (å…±å‡ºç° {count} æ¬¡)"


def multi_edit(operations: list[dict[str, Any]]) -> str:
    """æ‰¹é‡ç¼–è¾‘å¤šä¸ªæ–‡ä»¶

    Args:
        operations: ç¼–è¾‘æ“ä½œåˆ—è¡¨ï¼Œæ¯ä¸ªæ“ä½œåŒ…å«ï¼š
            - type: "replace" | "edit_lines"
            - file: æ–‡ä»¶è·¯å¾„
            - ... å…¶ä»–å‚æ•°å–å†³äº type

    Returns:
        æ“ä½œç»“æœæè¿°

    Raises:
        ValueError: æ“ä½œå‚æ•°æ— æ•ˆ
        RuntimeError: ç¼–è¾‘å¤±è´¥ï¼ˆä¼šå›æ»šæ‰€æœ‰æ“ä½œï¼‰

    Example:
        >>> operations = [
        ...     {
        ...         "type": "replace",
        ...         "file": "chapters/ch001.md",
        ...         "search": "å¼ ä¸‰",
        ...         "replace": "æå››"
        ...     },
        ...     {
        ...         "type": "replace",
        ...         "file": "chapters/ch002.md",
        ...         "search": "å¼ ä¸‰",
        ...         "replace": "æå››"
        ...     },
        ... ]
        >>> multi_edit(operations)
    """
    if not operations:
        return "âš ï¸ æ²¡æœ‰æ“ä½œéœ€è¦æ‰§è¡Œ"

    logger.info(f"å¼€å§‹æ‰¹é‡ç¼–è¾‘ï¼š{len(operations)} ä¸ªæ“ä½œ")

    # å¤‡ä»½æ‰€æœ‰æ–‡ä»¶
    backups: dict[str, str] = {}
    modified_files: list[str] = []

    try:
        # ç¬¬ä¸€æ­¥ï¼šå¤‡ä»½æ‰€æœ‰æ–‡ä»¶
        for op in operations:
            file_path = op.get("file")
            if not file_path:
                raise ValueError(f"æ“ä½œç¼ºå°‘ 'file' å‚æ•°: {op}")

            path = Path(file_path)
            if path.exists() and file_path not in backups:
                backups[file_path] = path.read_text(encoding="utf-8")

        # ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œæ‰€æœ‰æ“ä½œ
        for i, op in enumerate(operations, 1):
            op_type = op.get("type")
            file_path = op["file"]

            logger.debug(f"æ‰§è¡Œæ“ä½œ {i}/{len(operations)}: {op_type} on {file_path}")

            if op_type == "replace":
                replace_in_file(
                    file_path,
                    op["search"],
                    op["replace"],
                    op.get("occurrence"),
                )
                modified_files.append(file_path)

            elif op_type == "edit_lines":
                # æå–ç« èŠ‚ç¼–å·
                chapter_number = op.get("chapter_number")
                if chapter_number is None:
                    # å°è¯•ä»æ–‡ä»¶åæå–
                    match = Path(file_path).stem
                    if match.startswith("ch") and match[2:5].isdigit():
                        chapter_number = int(match[2:5])
                    else:
                        raise ValueError(f"æ— æ³•ç¡®å®šç« èŠ‚ç¼–å·: {file_path}")

                edit_chapter_lines(
                    chapter_number,
                    op["start_line"],
                    op["end_line"],
                    op["new_content"],
                    base_dir=str(Path(file_path).parent),
                )
                modified_files.append(file_path)

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {op_type}")

        logger.info(f"âœ… æ‰¹é‡ç¼–è¾‘å®Œæˆï¼šä¿®æ”¹äº† {len(set(modified_files))} ä¸ªæ–‡ä»¶")
        return (
            f"âœ… æ‰¹é‡ç¼–è¾‘å®Œæˆï¼šä¿®æ”¹äº† {len(set(modified_files))} ä¸ªæ–‡ä»¶ ({len(operations)} ä¸ªæ“ä½œ)"
        )

    except Exception as e:
        # å›æ»šæ‰€æœ‰ä¿®æ”¹
        logger.error(f"æ‰¹é‡ç¼–è¾‘å¤±è´¥ï¼Œæ­£åœ¨å›æ»š: {e}")

        for file_path, backup_content in backups.items():
            try:
                Path(file_path).write_text(backup_content, encoding="utf-8")
                logger.debug(f"å·²å›æ»š: {file_path}")
            except Exception as rollback_err:
                logger.error(f"å›æ»šå¤±è´¥: {file_path} - {rollback_err}")

        raise RuntimeError(f"æ‰¹é‡ç¼–è¾‘å¤±è´¥ï¼Œå·²å›æ»šæ‰€æœ‰ä¿®æ”¹: {e}") from e


# ========== å›¾æŸ¥è¯¢å·¥å…· (Graph Query Tools) ==========


def smart_context_search_tool(
    query: str,
    search_type: str = "all",
    max_hops: int = 2,
    limit: int = 10,
) -> str:
    """æ™ºèƒ½ä¸Šä¸‹æ–‡æœç´¢ï¼ˆåŸºäºå›¾æ•°æ®åº“ï¼‰

    ä½¿ç”¨ NervusDB å›¾æ•°æ®åº“è¿›è¡Œæ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œæ¯”å‘é‡æ£€ç´¢æ›´ç²¾å‡†ã€æ›´å¯è§£é‡Šã€‚

    Args:
        query: æœç´¢æŸ¥è¯¢ï¼ˆå¦‚"å¼ ä¸‰å’Œæå››çš„å…³ç³»"ï¼‰
        search_type: 'character' | 'location' | 'event' | 'foreshadow' | 'all'
        max_hops: æœ€å¤§å…³ç³»è·³æ•°ï¼ˆ1-3ï¼Œé»˜è®¤ 2ï¼‰
        limit: æœ€å¤šè¿”å›ç»“æœæ•°ï¼ˆé»˜è®¤ 10ï¼‰

    Returns:
        æ ¼å¼åŒ–çš„æœç´¢ç»“æœï¼ŒåŒ…å«ï¼š
        - ç›´æ¥åŒ¹é…çš„å®ä½“
        - é€šè¿‡å…³ç³»å…³è”çš„å®ä½“
        - å›¾è·¯å¾„å’Œç½®ä¿¡åº¦
        - ç»Ÿè®¡ä¿¡æ¯

    Example:
        >>> smart_context_search_tool("å¼ ä¸‰", "character", max_hops=2, limit=5)
        æ‰¾åˆ° 5 ä¸ªç›¸å…³ç»“æœï¼š
          - character: 2 ä¸ª
          - chapter: 3 ä¸ª

        1. [ç›´æ¥åŒ¹é…] å¼ ä¸‰ (character)
           ç½®ä¿¡åº¦: 1.0

        2. [1 è·³å…³ç³»] æå›› (character)
           è·¯å¾„: å¼ ä¸‰ -> æå››
           ç½®ä¿¡åº¦: 0.5

        ...
    """
    from .graph_query import smart_context_search

    logger.info(f"å›¾æŸ¥è¯¢: {query}, ç±»å‹={search_type}, è·³æ•°={max_hops}")

    try:
        db_path = os.getenv("NOVEL_GRAPH_DB", "data/novel-graph.nervusdb")
        result = smart_context_search(
            query=query,
            db_path=db_path,
            search_type=search_type,  # type: ignore
            max_hops=max_hops,
            limit=limit,
        )

        # æ ¼å¼åŒ–è¾“å‡º
        output = [result["summary"], ""]

        for i, item in enumerate(result["results"], 1):
            output.append(f"{i}. [{item['relevance']}] {item['name']} ({item['type']})")
            if item["path"] and len(item["path"]) > 1:
                output.append(f"   è·¯å¾„: {' -> '.join(item['path'])}")
            output.append(f"   ç½®ä¿¡åº¦: {item['confidence']:.2f}")
            output.append("")

        # æ·»åŠ ç»Ÿè®¡
        stats = result["graph_stats"]
        output.append(
            f"ğŸ“Š ç»Ÿè®¡: æœç´¢äº† {stats['nodes_searched']} ä¸ªèŠ‚ç‚¹ï¼Œæœ€å¤§æ·±åº¦ {stats['max_depth']}"
        )

        return "\n".join(output)

    except Exception as e:
        logger.error(f"å›¾æŸ¥è¯¢å¤±è´¥: {e}")
        return f"âŒ å›¾æŸ¥è¯¢å¤±è´¥: {e}\næç¤ºï¼šè¯·å…ˆè¿è¡Œ 'novel-agent build-graph' æ„å»ºå›¾æ•°æ®åº“"


def build_character_network_tool(character_names: str | None = None) -> str:
    """æ„å»ºè§’è‰²å…³ç³»ç½‘ç»œå›¾

    åˆ†æè§’è‰²ä¹‹é—´çš„å…³ç³»ï¼Œæ„å»ºç¤¾äº¤ç½‘ç»œå›¾ã€‚

    Args:
        character_names: è§’è‰²ååˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚"å¼ ä¸‰,æå››,ç‹äº”"ï¼‰
                        ç•™ç©ºåˆ™åˆ†ææ‰€æœ‰è§’è‰²

    Returns:
        æ ¼å¼åŒ–çš„ç½‘ç»œä¿¡æ¯ï¼š
        - èŠ‚ç‚¹ï¼ˆè§’è‰²ï¼‰åˆ—è¡¨
        - è¾¹ï¼ˆå…³ç³»ï¼‰åˆ—è¡¨
        - ç¤¾åŒºï¼ˆç¾¤ç»„ï¼‰æ£€æµ‹ç»“æœ

    Example:
        >>> build_character_network_tool("å¼ ä¸‰,æå››")
        è§’è‰²ç½‘ç»œåˆ†æç»“æœï¼š

        èŠ‚ç‚¹ (2 ä¸ª):
        1. å¼ ä¸‰ (protagonist)
        2. æå›› (supporting)

        å…³ç³» (1 æ¡):
        1. å¼ ä¸‰ -[knows]-> æå›› (å¼ºåº¦: 0.9)

        ç¤¾åŒº (1 ä¸ª):
        - ç¤¾åŒº 1: å¼ ä¸‰, æå›› (2 äºº)
    """
    from .graph_query import build_character_network

    logger.info(f"æ„å»ºè§’è‰²ç½‘ç»œ: {character_names or 'æ‰€æœ‰è§’è‰²'}")

    try:
        db_path = os.getenv("NOVEL_GRAPH_DB", "data/novel-graph.nervusdb")
        names_list = (
            [n.strip() for n in character_names.split(",") if n.strip()]
            if character_names
            else None
        )

        result = build_character_network(db_path=db_path, character_names=names_list)

        # æ ¼å¼åŒ–è¾“å‡º
        output = ["è§’è‰²ç½‘ç»œåˆ†æç»“æœï¼š", ""]

        # èŠ‚ç‚¹
        output.append(f"èŠ‚ç‚¹ ({len(result['nodes'])} ä¸ª):")
        for i, node in enumerate(result["nodes"][:20], 1):  # é™åˆ¶æ˜¾ç¤ºå‰ 20 ä¸ª
            node_type = node.get("properties", {}).get("type", node["type"])
            output.append(f"{i}. {node['label']} ({node_type})")
        if len(result["nodes"]) > 20:
            output.append(f"... è¿˜æœ‰ {len(result['nodes']) - 20} ä¸ªèŠ‚ç‚¹")
        output.append("")

        # å…³ç³»
        output.append(f"å…³ç³» ({len(result['edges'])} æ¡):")
        for i, edge in enumerate(result["edges"][:20], 1):
            weight = edge.get("weight", 1.0)
            relation = f"{edge['source']} -[{edge['relation']}]-> {edge['target']}"
            output.append(f"{i}. {relation} (å¼ºåº¦: {weight:.2f})")
        if len(result["edges"]) > 20:
            output.append(f"... è¿˜æœ‰ {len(result['edges']) - 20} æ¡å…³ç³»")
        output.append("")

        # ç¤¾åŒº
        output.append(f"ç¤¾åŒº ({len(result['clusters'])} ä¸ª):")
        for cluster in result["clusters"][:10]:
            members_str = ", ".join(cluster["members"][:5])
            if len(cluster["members"]) > 5:
                members_str += f" ... å…± {cluster['size']} äºº"
            output.append(f"- {cluster['label']}: {members_str}")
        if len(result["clusters"]) > 10:
            output.append(f"... è¿˜æœ‰ {len(result['clusters']) - 10} ä¸ªç¤¾åŒº")

        return "\n".join(output)

    except Exception as e:
        logger.error(f"æ„å»ºè§’è‰²ç½‘ç»œå¤±è´¥: {e}")
        return f"âŒ æ„å»ºè§’è‰²ç½‘ç»œå¤±è´¥: {e}\næç¤ºï¼šè¯·å…ˆè¿è¡Œ 'novel-agent build-graph' æ„å»ºå›¾æ•°æ®åº“"


def trace_foreshadow_tool(foreshadow_id: str) -> str:
    """è¿½æº¯ä¼ç¬”å®Œæ•´é“¾æ¡

    è¿½è¸ªä¼ç¬”ä»åŸ‹ä¸‹åˆ°æ­æ™“çš„å®Œæ•´è¿‡ç¨‹ã€‚

    Args:
        foreshadow_id: ä¼ç¬” IDï¼ˆå¦‚ "foreshadow_001"ï¼‰

    Returns:
        æ ¼å¼åŒ–çš„ä¼ç¬”è¿½æº¯ç»“æœï¼š
        - Setupï¼ˆåŸ‹ç¬”ï¼‰ç« èŠ‚
        - Hintsï¼ˆæš—ç¤ºï¼‰åˆ—è¡¨
        - Revealï¼ˆæ­æ™“ï¼‰ç« èŠ‚
        - çŠ¶æ€ï¼ˆå·²è§£å†³/æœªè§£å†³ï¼‰

    Example:
        >>> trace_foreshadow_tool("foreshadow_001")
        ä¼ç¬”è¿½æº¯: foreshadow_001

        ğŸ“ åŸ‹ç¬” (Setup):
        - ç¬¬ 5 ç« 

        ğŸ’¡ æš—ç¤º (Hints):
        - ç¬¬ 5 ç« : é¦–æ¬¡æåŠ
        - ç¬¬ 8 ç« : éšæ™¦æš—ç¤º
        - ç¬¬ 12 ç« : æ˜ç¡®æš—ç¤º

        ğŸ¯ æ­æ™“ (Reveal):
        - ç¬¬ 20 ç« 

        âœ… çŠ¶æ€: å·²è§£å†³
    """
    from .graph_query import trace_foreshadow

    logger.info(f"è¿½æº¯ä¼ç¬”: {foreshadow_id}")

    try:
        db_path = os.getenv("NOVEL_GRAPH_DB", "data/novel-graph.nervusdb")
        result = trace_foreshadow(foreshadow_id=foreshadow_id, db_path=db_path)

        if "error" in result:
            return f"âŒ {result['error']}"

        # æ ¼å¼åŒ–è¾“å‡º
        output = [f"ä¼ç¬”è¿½æº¯: {foreshadow_id}", ""]

        # Setup
        if result.get("setup"):
            setup = result["setup"]
            output.append("ğŸ“ åŸ‹ç¬” (Setup):")
            output.append(f"- ç¬¬ {setup['chapter']} ç« ")
            output.append("")

        # Hints
        hints = result.get("hints", [])
        if hints:
            output.append(f"ğŸ’¡ æš—ç¤º (Hints, {len(hints)} å¤„):")
            for hint in hints:
                output.append(f"- ç¬¬ {hint['chapter']} ç« ")
            output.append("")

        # Reveal
        if result.get("reveal"):
            reveal = result["reveal"]
            output.append("ğŸ¯ æ­æ™“ (Reveal):")
            output.append(f"- ç¬¬ {reveal['chapter']} ç« ")
            output.append("")

        # Status
        status_emoji = "âœ…" if result["status"] == "resolved" else "âš ï¸ "
        status_text = "å·²è§£å†³" if result["status"] == "resolved" else "æœªè§£å†³"
        output.append(f"{status_emoji} çŠ¶æ€: {status_text}")

        return "\n".join(output)

    except Exception as e:
        logger.error(f"è¿½æº¯ä¼ç¬”å¤±è´¥: {e}")
        return f"âŒ è¿½æº¯ä¼ç¬”å¤±è´¥: {e}\næç¤ºï¼šè¯·å…ˆè¿è¡Œ 'novel-agent build-graph' æ„å»ºå›¾æ•°æ®åº“"


def read_multiple_files(paths: str) -> str:
    """æ‰¹é‡è¯»å–å¤šä¸ªæ–‡ä»¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰

    ä¸€æ¬¡æ€§è¯»å–å¤šä¸ªæ–‡ä»¶ï¼Œå‡å°‘ API è°ƒç”¨æ¬¡æ•°

    Args:
        paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¦‚ "ch1.md,ch2.md,ch3.md"ï¼‰

    Returns:
        æ‰€æœ‰æ–‡ä»¶çš„å†…å®¹ï¼Œæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²

    Raises:
        FileNotFoundError: æŸä¸ªæ–‡ä»¶ä¸å­˜åœ¨
    """
    path_list = [p.strip() for p in paths.split(",")]
    logger.info(f"æ‰¹é‡è¯»å– {len(path_list)} ä¸ªæ–‡ä»¶")

    results = []
    for path in path_list:
        try:
            content = read_file(path)
            results.append(f"=== {path} ===\n{content}\n")
        except FileNotFoundError:
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {path}")
            results.append(f"=== {path} ===\nâŒ æ–‡ä»¶ä¸å­˜åœ¨\n")

    return "\n".join(results)


# å·¥å…·è£…é¥°å™¨åŒ…è£…ï¼ˆç”¨äº LangChainï¼‰
read_multiple_files_tool = lc_tool(read_multiple_files)


# ============================================================================
# å†™ä½œæ¨¡æ¿å·¥å…·
# ============================================================================


def list_templates(category: str | None = None) -> str:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å†™ä½œæ¨¡æ¿

    Args:
        category: å¯é€‰çš„åˆ†ç±»è¿‡æ»¤ï¼ˆscene/dialogue/action/psychology/transitionï¼‰

    Returns:
        æ ¼å¼åŒ–çš„æ¨¡æ¿åˆ—è¡¨

    Example:
        >>> list_templates()
        >>> list_templates(category="action")
    """
    templates_dir = Path("spec/templates")
    if not templates_dir.exists():
        return "âŒ æ¨¡æ¿ç›®å½•ä¸å­˜åœ¨ï¼šspec/templates/"

    # è·å–æ‰€æœ‰æ¨¡æ¿æ–‡ä»¶
    template_files = sorted(templates_dir.glob("*.md"))
    if not template_files:
        return "âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡æ¿æ–‡ä»¶"

    templates = []
    for file_path in template_files:
        try:
            # è§£æ frontmatter
            with open(file_path, encoding="utf-8") as f:
                post = frontmatter.load(f)

            template_name = file_path.stem
            template_category = post.get("category", "unknown")
            template_description = post.get("description", "")

            # åˆ†ç±»è¿‡æ»¤
            if category and template_category != category:
                continue

            templates.append(
                {
                    "name": template_name,
                    "category": template_category,
                    "description": template_description,
                    "display_name": post.get("name", template_name),
                }
            )
        except Exception as e:
            logger.warning(f"è§£ææ¨¡æ¿å¤±è´¥ {file_path}: {e}")
            continue

    if not templates:
        if category:
            return f"âŒ æœªæ‰¾åˆ°åˆ†ç±»ä¸º '{category}' çš„æ¨¡æ¿"
        return "âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ¨¡æ¿"

    # æ ¼å¼åŒ–è¾“å‡º
    lines = ["å¯ç”¨çš„å†™ä½œæ¨¡æ¿ï¼š\n"]
    if category:
        lines[0] = f"å¯ç”¨çš„å†™ä½œæ¨¡æ¿ï¼ˆåˆ†ç±»ï¼š{category}ï¼‰ï¼š\n"

    # æŒ‰åˆ†ç±»åˆ†ç»„
    by_category: dict[str, list[dict[str, str]]] = {}
    for t in templates:
        cat = t["category"]
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(t)

    for cat, temps in sorted(by_category.items()):
        lines.append(f"## {cat.upper()}")
        for t in temps:
            lines.append(f"- **{t['name']}**: {t['display_name']}")
            if t["description"]:
                lines.append(f"  {t['description']}")
        lines.append("")

    return "\n".join(lines)


def apply_template(template_name: str, variables: dict[str, str]) -> str:
    """åº”ç”¨å†™ä½œæ¨¡æ¿ï¼Œä½¿ç”¨æä¾›çš„å˜é‡æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦

    Args:
        template_name: æ¨¡æ¿åç§°ï¼ˆä¸å« .md åç¼€ï¼‰
        variables: å˜é‡å­—å…¸ï¼Œkey ä¸ºå˜é‡åï¼Œvalue ä¸ºæ›¿æ¢å€¼

    Returns:
        æ›¿æ¢åçš„æ–‡æœ¬å†…å®¹

    Example:
        >>> apply_template("scene-description", {
        ...     "time": "é»„æ˜",
        ...     "location": "è’å‡‰çš„æˆ˜åœºä¸Š",
        ...     "weather": "ä¹Œäº‘å¯†å¸ƒ"
        ... })
    """
    templates_dir = Path("spec/templates")
    template_file = templates_dir / f"{template_name}.md"

    if not template_file.exists():
        available = list_templates()
        return f"âŒ æ¨¡æ¿ä¸å­˜åœ¨ï¼š{template_name}\n\n{available}"

    try:
        # è§£ææ¨¡æ¿
        with open(template_file, encoding="utf-8") as f:
            post = frontmatter.load(f)

        content: str = str(post.content)

        # æå–æ¨¡æ¿å†…å®¹ï¼ˆå»æ‰ä½¿ç”¨ç¤ºä¾‹éƒ¨åˆ†ï¼‰
        if "---\n\n**ä½¿ç”¨ç¤ºä¾‹**ï¼š" in content:
            content = content.split("---\n\n**ä½¿ç”¨ç¤ºä¾‹**ï¼š")[0].strip()

        # å˜é‡æ›¿æ¢
        for key, value in variables.items():
            pattern = rf"\$\{{{key}\}}"
            content = re.sub(pattern, value, content)

        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªæ›¿æ¢çš„å˜é‡
        remaining_vars = re.findall(r"\$\{([^}]+)\}", content)
        if remaining_vars:
            logger.warning(f"æ¨¡æ¿ä¸­æœ‰æœªæ›¿æ¢çš„å˜é‡: {remaining_vars}")
            content += f"\n\nâš ï¸  ä»¥ä¸‹å˜é‡æœªæä¾›å€¼ï¼š{', '.join(remaining_vars)}"

        return content.strip()

    except Exception as e:
        logger.error(f"åº”ç”¨æ¨¡æ¿å¤±è´¥: {e}")
        return f"âŒ åº”ç”¨æ¨¡æ¿å¤±è´¥ï¼š{str(e)}"


# ==================== é£æ ¼æŒ‡å—ç³»ç»Ÿ ====================


def check_style_compliance(chapter_number: int) -> str:
    """æ£€æŸ¥ç« èŠ‚æ˜¯å¦ç¬¦åˆé£æ ¼æŒ‡å—è¦æ±‚

    Args:
        chapter_number: ç« èŠ‚ç¼–å·

    Returns:
        æ ¼å¼åŒ–çš„æ£€æŸ¥æŠ¥å‘Šï¼ˆMarkdownï¼‰
    """
    from pathlib import Path

    import yaml

    try:
        # è¯»å–é£æ ¼æŒ‡å—
        style_guide_path = Path("spec/style-guide.yaml")
        if not style_guide_path.exists():
            return "âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° spec/style-guide.yaml é£æ ¼æŒ‡å—æ–‡ä»¶"

        with open(style_guide_path, encoding="utf-8") as f:
            style_guide = yaml.safe_load(f)

        # è¯»å–ç« èŠ‚å†…å®¹
        chapter_file = Path(f"chapters/chapter_{chapter_number}.md")
        if not chapter_file.exists():
            return f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç¬¬ {chapter_number} ç« æ–‡ä»¶"

        with open(chapter_file, encoding="utf-8") as f:
            content = f.read()

        lines = content.split("\n")

        # æ£€æŸ¥æŠ¥å‘Š
        report = [f"# ç¬¬{chapter_number}ç« é£æ ¼æ£€æŸ¥æŠ¥å‘Š\n"]

        # 1. æ£€æŸ¥ç¦ç”¨è¯æ±‡
        forbidden_issues = []
        forbidden_words = style_guide.get("forbidden_words", [])
        for line_num, line in enumerate(lines, 1):
            for rule in forbidden_words:
                word = rule["word"]
                if word in line:
                    suggestions = ", ".join(
                        f'"{s}"' if s else "(åˆ é™¤)" for s in rule["suggestions"][:3]
                    )
                    forbidden_issues.append(
                        f'- ç¬¬{line_num}è¡Œï¼š"{line.strip()}" â†’ å«æœ‰ "{word}" '
                        f"({rule['reason']})\\n  å»ºè®®: {suggestions}"
                    )

        if forbidden_issues:
            report.append(f"## âŒ ç¦ç”¨è¯æ±‡ï¼ˆ{len(forbidden_issues)}å¤„ï¼‰\n")
            report.extend(forbidden_issues)
            report.append("")
        else:
            report.append("## âœ… ç¦ç”¨è¯æ±‡ï¼šé€šè¿‡\n")

        # 2. æ£€æŸ¥è§’è‰²è¯­æ°”
        voice_issues = []
        character_voice = style_guide.get("character_voice", {})
        for line_num, line in enumerate(lines, 1):
            # æ£€æµ‹å¯¹è¯ï¼ˆç®€å•å®ç°ï¼šåŒ…å«å†’å·æˆ–å¼•å·ï¼‰
            if "ï¼š" in line or '"' in line or "ã€Œ" in line:
                for char_name, char_rules in character_voice.items():
                    if char_name in line:
                        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ç¦ç”¨è¯æ±‡
                        for forbidden in char_rules.get("forbidden", []):
                            if forbidden in line:
                                voice_issues.append(
                                    f'- ç¬¬{line_num}è¡Œï¼š{char_name}è¯´ "{line.strip()}" '
                                    f'â†’ å«æœ‰ä¸ç¬¦åˆè§’è‰²è®¾å®šçš„è¯æ±‡ "{forbidden}"\\n'
                                    f"  {char_name}çš„è¯­æ°”åº”ä¸ºï¼š{char_rules['tone']}"
                                )

        if voice_issues:
            report.append(f"## âš ï¸  è§’è‰²è¯­æ°”ä¸ä¸€è‡´ï¼ˆ{len(voice_issues)}å¤„ï¼‰\n")
            report.extend(voice_issues)
            report.append("")
        else:
            report.append("## âœ… è§’è‰²è¯­æ°”ï¼šé€šè¿‡\n")

        # 3. æ£€æŸ¥æ ‡ç‚¹ç¬¦å·
        punct_issues = []
        punct_rules = style_guide.get("punctuation_rules", {})

        # æ£€æŸ¥æ„Ÿå¹å·æ•°é‡
        exclaim_count = content.count("ï¼")
        exclaim_limit = punct_rules.get("exclamation_limit", 5)
        if exclaim_count > exclaim_limit:
            punct_issues.append(f"- æ„Ÿå¹å·è¿‡å¤šï¼š{exclaim_count}å¤„ï¼ˆé™åˆ¶ï¼š{exclaim_limit}å¤„ï¼‰")

        # æ£€æŸ¥çœç•¥å·æ ¼å¼
        if "..." in content:
            punct_issues.append(
                f"- çœç•¥å·æ ¼å¼é”™è¯¯ï¼šåº”ä½¿ç”¨ \"{punct_rules.get('ellipsis_format', 'â€¦â€¦')}\""
                ' è€Œé "..."'
            )

        if punct_issues:
            report.append(f"## âš ï¸  æ ‡ç‚¹ç¬¦å·è§„èŒƒï¼ˆ{len(punct_issues)}å¤„ï¼‰\n")
            report.extend(punct_issues)
            report.append("")
        else:
            report.append("## âœ… æ ‡ç‚¹ç¬¦å·è§„èŒƒï¼šé€šè¿‡\n")

        # 4. æ£€æŸ¥å¥å¼é£æ ¼
        style_issues = []
        sentence_style = style_guide.get("sentence_style", {})
        max_length = sentence_style.get("max_length", 50)

        for line_num, line in enumerate(lines, 1):
            # æ£€æŸ¥å¥å­é•¿åº¦ï¼ˆç®€å•å®ç°ï¼šæŒ‰å¥å·åˆ†å‰²ï¼‰
            sentences = [s for s in line.split("ã€‚") if s.strip()]
            for sent in sentences:
                if len(sent) > max_length:
                    style_issues.append(
                        f"- ç¬¬{line_num}è¡Œï¼šå¥å­è¿‡é•¿ï¼ˆ{len(sent)}å­—ï¼Œé™åˆ¶{max_length}å­—ï¼‰"
                    )

        if style_issues:
            report.append(f"## âš ï¸  å¥å¼é£æ ¼ï¼ˆ{len(style_issues)}å¤„ï¼‰\n")
            report.extend(style_issues)
            report.append("")
        else:
            report.append("## âœ… å¥å¼é£æ ¼ï¼šé€šè¿‡\n")

        # æ€»ç»“
        total_issues = (
            len(forbidden_issues) + len(voice_issues) + len(punct_issues) + len(style_issues)
        )
        if total_issues == 0:
            report.append("\n## ğŸ‰ æ€»ç»“\n\næ‰€æœ‰æ£€æŸ¥é¡¹å‡é€šè¿‡ï¼")
        else:
            report.append(f"\n## ğŸ“Š æ€»ç»“\n\nå‘ç° {total_issues} å¤„éœ€è¦æ”¹è¿›çš„åœ°æ–¹ã€‚")

        return "\n".join(report)

    except Exception as e:
        logger.error(f"é£æ ¼æ£€æŸ¥å¤±è´¥: {e}")
        return f"âŒ é£æ ¼æ£€æŸ¥å¤±è´¥ï¼š{str(e)}"


def apply_style_fix(chapter_number: int, auto_fix: bool = False) -> str:
    """åº”ç”¨é£æ ¼ä¿®å¤å»ºè®®

    Args:
        chapter_number: ç« èŠ‚ç¼–å·
        auto_fix: æ˜¯å¦è‡ªåŠ¨ä¿®å¤ï¼ˆTrue=è‡ªåŠ¨ä¿®å¤ï¼ŒFalse=ä»…æ˜¾ç¤ºå»ºè®®ï¼‰

    Returns:
        ä¿®å¤æŠ¥å‘Šæˆ–å»ºè®®åˆ—è¡¨
    """
    from pathlib import Path

    import yaml

    try:
        # è¯»å–é£æ ¼æŒ‡å—
        style_guide_path = Path("spec/style-guide.yaml")
        if not style_guide_path.exists():
            return "âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° spec/style-guide.yaml é£æ ¼æŒ‡å—æ–‡ä»¶"

        with open(style_guide_path, encoding="utf-8") as f:
            style_guide = yaml.safe_load(f)

        # è¯»å–ç« èŠ‚å†…å®¹
        chapter_file = Path(f"chapters/chapter_{chapter_number}.md")
        if not chapter_file.exists():
            return f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç¬¬ {chapter_number} ç« æ–‡ä»¶"

        with open(chapter_file, encoding="utf-8") as f:
            content = f.read()

        if not auto_fix:
            # ä»…æ˜¾ç¤ºå»ºè®®
            report = check_style_compliance(chapter_number)
            return f"{report}\n\nğŸ’¡ æç¤ºï¼šä½¿ç”¨ auto_fix=True å‚æ•°å¯è‡ªåŠ¨åº”ç”¨ä¿®å¤å»ºè®®"

        # è‡ªåŠ¨ä¿®å¤
        modified = content
        fixes_applied = []

        # 1. ä¿®å¤ç¦ç”¨è¯æ±‡ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå»ºè®®ï¼‰
        forbidden_words = style_guide.get("forbidden_words", [])
        for rule in forbidden_words:
            word = rule["word"]
            suggestions = rule.get("suggestions", [])
            if suggestions and suggestions[0]:  # ä½¿ç”¨ç¬¬ä¸€ä¸ªéç©ºå»ºè®®
                replacement = suggestions[0]
                count = modified.count(word)
                if count > 0:
                    modified = modified.replace(word, replacement)
                    fixes_applied.append(f'- æ›¿æ¢ "{word}" â†’ "{replacement}" ({count}å¤„)')
            elif word in modified:  # å¦‚æœç¬¬ä¸€ä¸ªå»ºè®®æ˜¯ç©ºï¼ˆåˆ é™¤ï¼‰
                count = modified.count(word)
                modified = modified.replace(word, "")
                fixes_applied.append(f'- åˆ é™¤ "{word}" ({count}å¤„)')

        # 2. ä¿®å¤çœç•¥å·æ ¼å¼
        punct_rules = style_guide.get("punctuation_rules", {})
        ellipsis_format = punct_rules.get("ellipsis_format", "â€¦â€¦")
        if "..." in modified:
            count = modified.count("...")
            modified = modified.replace("...", ellipsis_format)
            fixes_applied.append(f"- ä¿®å¤çœç•¥å·æ ¼å¼ ({count}å¤„)")

        # ä¿å­˜ä¿®æ”¹
        if fixes_applied:
            with open(chapter_file, "w", encoding="utf-8") as f:
                f.write(modified)

            fix_report: list[str] = [
                f"# ç¬¬{chapter_number}ç« é£æ ¼ä¿®å¤æŠ¥å‘Š\n",
                f"## âœ… å·²åº”ç”¨ä¿®å¤ï¼ˆ{len(fixes_applied)}é¡¹ï¼‰\n",
            ]
            fix_report.extend(fixes_applied)
            fix_report.append("\n## ğŸ“ æç¤º\n")
            fix_report.append("- æ–‡ä»¶å·²ä¿å­˜")
            fix_report.append("- å»ºè®®é‡æ–°è¿è¡Œæ£€æŸ¥ç¡®è®¤æ•ˆæœ")
            return "\n".join(fix_report)
        else:
            return f"ç¬¬{chapter_number}ç« æ²¡æœ‰å¯è‡ªåŠ¨ä¿®å¤çš„é—®é¢˜ã€‚"

    except Exception as e:
        logger.error(f"é£æ ¼ä¿®å¤å¤±è´¥: {e}")
        return f"âŒ é£æ ¼ä¿®å¤å¤±è´¥ï¼š{str(e)}"


# ==================== å¤§çº²ç”Ÿæˆå™¨ ====================


def generate_outline(
    genre: str,
    target_words: int,
    themes: list[str],
    style: str = "çˆ½æ–‡",
) -> str:
    """æ ¹æ®é¢˜æè‡ªåŠ¨ç”Ÿæˆä¸‰å¹•ç»“æ„å¤§çº²

    Args:
        genre: é¢˜æï¼ˆç„å¹»ã€éƒ½å¸‚ã€ç§‘å¹»ã€æ­¦ä¾ ã€å†å²ã€è¨€æƒ…ï¼‰
        target_words: ç›®æ ‡å­—æ•°
        themes: ä¸»é¢˜åˆ—è¡¨
        style: é£æ ¼ï¼ˆçˆ½æ–‡ã€è™æ–‡ã€è½»æ¾ã€ä¸¥è‚ƒï¼‰

    Returns:
        æ ¼å¼åŒ–çš„å¤§çº²æ–‡æœ¬ï¼ˆMarkdownï¼‰
    """
    try:
        # è®¡ç®—ç« èŠ‚æ•°ï¼ˆå‡è®¾æ¯ç« 1000å­—ï¼‰
        words_per_chapter = 1000
        total_chapters = max(target_words // words_per_chapter, 10)

        # ä¸‰å¹•åˆ†é…ï¼ˆèµ·10%ã€æ‰¿è½¬70%ã€åˆ20%ï¼‰
        act1_chapters = max(int(total_chapters * 0.1), 5)
        act3_chapters = max(int(total_chapters * 0.2), 10)
        act2_chapters = total_chapters - act1_chapters - act3_chapters

        act1_words = act1_chapters * words_per_chapter
        act2_words = act2_chapters * words_per_chapter
        act3_words = act3_chapters * words_per_chapter

        # ä¸»é¢˜æ–‡æœ¬
        themes_text = "ã€".join(themes)

        # ç”Ÿæˆå¤§çº²
        outline = [
            f"# {genre}å°è¯´å¤§çº²ï¼ˆ{target_words}å­—ï¼Œçº¦{total_chapters}ç« ï¼‰\n",
            "## åŸºæœ¬ä¿¡æ¯\n",
            f"- **é¢˜æ**ï¼š{genre}",
            f"- **é£æ ¼**ï¼š{style}",
            f"- **ä¸»é¢˜**ï¼š{themes_text}",
            f"- **ç›®æ ‡å­—æ•°**ï¼š{target_words}å­—",
            f"- **ç« èŠ‚æ•°**ï¼š{total_chapters}ç« ï¼ˆæ¯ç« çº¦{words_per_chapter}å­—ï¼‰\n",
            f"## ç¬¬ä¸€å¹•ï¼šå»ºç«‹ä¸–ç•Œè§‚ï¼ˆ1-{act1_chapters}ç« ï¼Œ{act1_words}å­—ï¼‰\n",
            "### æ ¸å¿ƒç›®æ ‡",
            "- å¼•å…¥ä¸»è§’å’Œä¸–ç•Œè§‚",
            "- å»ºç«‹å†²çªå’ŒåŠ¨æœº",
            "- å±•ç¤ºä¸»è§’åˆå§‹èƒ½åŠ›\n",
            "### ç« èŠ‚åˆ†é…",
            f"- **ç¬¬1ç« **ï¼šå¼€ç¯‡ï¼Œä¸»è§’ç™»åœºï¼ˆ{words_per_chapter}å­—ï¼‰",
            "  - æƒ…èŠ‚ç‚¹ï¼šä»‹ç»ä¸»è§’èƒŒæ™¯å’Œç°çŠ¶",
            "  - å†²çªï¼šå¼•å…¥åˆå§‹çŸ›ç›¾",
            "",
            f"- **ç¬¬{act1_chapters // 2}ç« **ï¼šäº‹ä»¶è§¦å‘ï¼Œä¸»è§’å·å…¥ï¼ˆ{words_per_chapter}å­—ï¼‰",
            "  - æƒ…èŠ‚ç‚¹ï¼šå…³é”®äº‹ä»¶å‘ç”Ÿ",
            "  - è½¬æŠ˜ï¼šä¸»è§’è¢«è¿«æ”¹å˜",
            "",
            f"- **ç¬¬{act1_chapters}ç« **ï¼šç¬¬ä¸€å¹•ç»“æŸï¼Œè¿›å…¥æ–°ä¸–ç•Œï¼ˆ{words_per_chapter}å­—ï¼‰",
            "  - æƒ…èŠ‚ç‚¹ï¼šä¸»è§’è¸ä¸Šæ—…ç¨‹",
            "  - ç»“æœï¼šç¦»å¼€èˆ’é€‚åŒº\n",
            (
                f"## ç¬¬äºŒå¹•ï¼šå†²çªå‡çº§ï¼ˆ{act1_chapters + 1}-"
                f"{act1_chapters + act2_chapters}ç« ï¼Œ{act2_words}å­—ï¼‰\n"
            ),
            "### æ ¸å¿ƒç›®æ ‡",
            "- ä¸»è§’æˆé•¿å’Œå†ç»ƒ",
            "- å¼•å…¥æ›´å¤§çš„å†²çª",
            "- æƒ…èŠ‚ä¸æ–­å‡çº§\n",
            "### å…³é”®æƒ…èŠ‚ç‚¹",
            f"- **ç¬¬{act1_chapters + act2_chapters // 4}ç« **ï¼šç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œé­é‡å¼ºæ•Œ",
            f"- **ç¬¬{act1_chapters + act2_chapters // 2}ç« **ï¼šè½¬æŠ˜ç‚¹ï¼Œè·å¾—æœºç¼˜æˆ–å‘ç°çœŸç›¸",
            f"- **ç¬¬{act1_chapters + act2_chapters * 3 // 4}ç« **ï¼šæƒ…èŠ‚å‡çº§ï¼Œé¢ä¸´é‡å¤§é€‰æ‹©",
            f"- **ç¬¬{act1_chapters + act2_chapters}ç« **ï¼šç¬¬äºŒå¹•ç»“æŸï¼Œå‡†å¤‡æœ€ç»ˆå†³æˆ˜\n",
            (
                f"## ç¬¬ä¸‰å¹•ï¼šé«˜æ½®ä¸ç»“å±€ï¼ˆ{act1_chapters + act2_chapters + 1}-"
                f"{total_chapters}ç« ï¼Œ{act3_words}å­—ï¼‰\n"
            ),
            "### æ ¸å¿ƒç›®æ ‡",
            "- æœ€ç»ˆå†³æˆ˜",
            "- è§£å†³æ‰€æœ‰å†²çª",
            "- è§’è‰²æˆé•¿å®Œæˆ\n",
            "### ç« èŠ‚åˆ†é…",
            f"- **ç¬¬{act1_chapters + act2_chapters + act3_chapters // 2}ç« **ï¼šæœ€ç»ˆå†³æˆ˜å¼€å§‹",
            f"- **ç¬¬{total_chapters - 2}ç« **ï¼šé«˜æ½®ï¼Œå†³å®šæ€§æ—¶åˆ»",
            f"- **ç¬¬{total_chapters}ç« **ï¼šå¤§ç»“å±€ï¼Œå¼€å¯æ–°ç¯‡ç« \n",
            "## è§’è‰²æˆé•¿å¼§çº¿\n",
            "1. **èµ·ç‚¹**ï¼šæ™®é€šäºº/å¼±è€…",
            "2. **è§‰é†’**ï¼šå‘ç°å¤©èµ‹/ä½¿å‘½",
            "3. **æˆé•¿**ï¼šå†ç»ƒå’Œçªç ´",
            "4. **è½¬æŠ˜**ï¼šé¢ä¸´å¤±è´¥å’ŒæŒ«æŠ˜",
            "5. **èœ•å˜**ï¼šå…‹æœå›°éš¾ï¼Œè¾¾æˆç›®æ ‡",
            "6. **ç»ˆç‚¹**ï¼šæˆä¸ºå¼ºè€…ï¼Œå®Œæˆä½¿å‘½\n",
            "## åˆ›ä½œå»ºè®®\n",
            f"- **èŠ‚å¥**ï¼š{style}é£æ ¼ï¼Œæ³¨æ„æƒ…èŠ‚èµ·ä¼",
            f"- **ä¸»é¢˜**ï¼šå›´ç»•{themes_text}å±•å¼€",
            "- **å†²çª**ï¼šç¡®ä¿æ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ˜ç¡®çš„çŸ›ç›¾",
            "- **æˆé•¿**ï¼šä¸»è§’èƒ½åŠ›å’Œå¿ƒç†è¦æœ‰æ˜æ˜¾å˜åŒ–",
        ]

        return "\n".join(outline)

    except Exception as e:
        logger.error(f"å¤§çº²ç”Ÿæˆå¤±è´¥: {e}")
        return f"âŒ å¤§çº²ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"


# å·¥å…·è£…é¥°å™¨åŒ…è£…ï¼ˆç”¨äº LangChainï¼‰
list_templates_tool = lc_tool(list_templates)
apply_template_tool = lc_tool(apply_template)
check_style_compliance_tool = lc_tool(check_style_compliance)
apply_style_fix_tool = lc_tool(apply_style_fix)
generate_outline_tool = lc_tool(generate_outline)

# åˆ›æ„å·¥å…·åŒ…è£…
dialogue_enhancer_tool = lc_tool(dialogue_enhancer)
plot_twist_generator_tool = lc_tool(plot_twist_generator)
scene_transition_tool = lc_tool(scene_transition)
