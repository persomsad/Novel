"""Novel Agent Tools

å®ç° 5 ä¸ªæ ¸å¿ƒå·¥å…·ï¼š
1. read_file - è¯»å–ä»»æ„æ–‡ä»¶
2. write_chapter - åˆ›å»ºæ–°ç« èŠ‚
3. search_content - æœç´¢å…³é”®è¯
4. verify_strict_timeline - æ—¶é—´çº¿ç²¾ç¡®éªŒè¯
5. verify_strict_references - å¼•ç”¨å®Œæ•´æ€§éªŒè¯
"""

import json
import os
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any

from langchain_core.tools import tool as lc_tool

from . import nervus_cli
from .logging_config import get_logger

logger = get_logger(__name__)


def read_file(path: str) -> str:
    """è¯»å–æ–‡ä»¶å†…å®¹

    Args:
        path: æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰

    Returns:
        æ–‡ä»¶å†…å®¹

    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        PermissionError: æ— è¯»å–æƒé™
    """
    file_path = Path(path)
    logger.debug(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {path}")

    if not file_path.exists():
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")

    try:
        content = file_path.read_text(encoding="utf-8")
        logger.info(f"æˆåŠŸè¯»å–æ–‡ä»¶: {path} ({len(content)} å­—ç¬¦)")
        return content
    except PermissionError:
        logger.error(f"æ— æƒé™è¯»å–æ–‡ä»¶: {path}")
        raise
    except Exception as e:
        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {path}, é”™è¯¯: {e}")
        raise


def write_chapter(number: int, content: str, base_dir: str = "chapters") -> str:
    """åˆ›å»ºæ–°ç« èŠ‚

    Args:
        number: ç« èŠ‚ç¼–å·ï¼ˆ1-999ï¼‰
        content: ç« èŠ‚å†…å®¹
        base_dir: ç« èŠ‚ç›®å½•ï¼ˆé»˜è®¤: chaptersï¼‰

    Returns:
        åˆ›å»ºçš„æ–‡ä»¶è·¯å¾„

    Raises:
        ValueError: ç« èŠ‚ç¼–å·æ— æ•ˆ
        OSError: æ–‡ä»¶ç³»ç»Ÿé”™è¯¯
    """
    logger.debug(f"æ­£åœ¨åˆ›å»ºç« èŠ‚: ç¼–å·={number}, ç›®å½•={base_dir}")

    if not 1 <= number <= 999:
        logger.error(f"æ— æ•ˆçš„ç« èŠ‚ç¼–å·: {number}")
        raise ValueError(f"ç« èŠ‚ç¼–å·å¿…é¡»åœ¨ 1-999 ä¹‹é—´ï¼Œå½“å‰: {number}")

    try:
        # åˆ›å»ºç›®å½•
        chapters_dir = Path(base_dir)
        chapters_dir.mkdir(parents=True, exist_ok=True)

        # æ ¼å¼åŒ–æ–‡ä»¶åï¼šch001.md, ch002.md, ...
        filename = f"ch{number:03d}.md"
        file_path = chapters_dir / filename

        # å†™å…¥å†…å®¹
        file_path.write_text(content, encoding="utf-8")

        logger.info(f"æˆåŠŸåˆ›å»ºç« èŠ‚: {file_path} ({len(content)} å­—ç¬¦)")
        return str(file_path)
    except OSError as e:
        logger.error(f"åˆ›å»ºç« èŠ‚å¤±è´¥: {e}")
        raise


def search_content(keyword: str, search_dir: str = ".") -> list[dict[str, str]]:
    """æœç´¢å…³é”®è¯

    ä½¿ç”¨ ripgrep åœ¨æŒ‡å®šç›®å½•æœç´¢å…³é”®è¯

    Args:
        keyword: æœç´¢å…³é”®è¯
        search_dir: æœç´¢ç›®å½•ï¼ˆé»˜è®¤: å½“å‰ç›®å½•ï¼‰

    Returns:
        åŒ¹é…ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«:
        - file: æ–‡ä»¶è·¯å¾„
        - line: è¡Œå·
        - content: åŒ¹é…å†…å®¹
    """
    logger.debug(f"æœç´¢å…³é”®è¯: '{keyword}' åœ¨ç›®å½•: {search_dir}")

    try:
        # ä½¿ç”¨ rg æœç´¢
        result = subprocess.run(
            ["rg", "--json", "--fixed-strings", keyword, search_dir],
            capture_output=True,
            text=True,
            check=False,  # ä¸æŠ›å‡ºå¼‚å¸¸ï¼ˆæ²¡æœ‰åŒ¹é…æ—¶ rg è¿”å› 1ï¼‰
        )

        if result.returncode not in (0, 1):
            # å…¶ä»–é”™è¯¯ï¼ˆ2=æœç´¢é”™è¯¯ï¼‰
            logger.error(f"ripgrepæœç´¢å¤±è´¥: {result.stderr}")
            raise RuntimeError(f"æœç´¢å¤±è´¥: {result.stderr}")

        # è§£æ JSON è¾“å‡º
        import json

        matches: list[dict[str, str]] = []
        for line in result.stdout.strip().split("\n"):
            if not line:
                continue
            data = json.loads(line)
            if data.get("type") == "match":
                match_data = data["data"]
                matches.append(
                    {
                        "file": match_data["path"]["text"],
                        "line": str(match_data["line_number"]),
                        "content": match_data["lines"]["text"].strip(),
                    }
                )

        logger.info(f"æœç´¢å®Œæˆ: æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
        return matches

    except FileNotFoundError:
        # ripgrep æœªå®‰è£…ï¼Œå›é€€åˆ° Python å®ç°
        logger.warning("ripgrepæœªå®‰è£…ï¼Œä½¿ç”¨Python fallbackå®ç°")
        return _search_content_fallback(keyword, search_dir)


def _search_content_fallback(keyword: str, search_dir: str) -> list[dict[str, str]]:
    """æœç´¢å…³é”®è¯ï¼ˆPython å®ç°ä½œä¸ºåå¤‡ï¼‰"""
    matches: list[dict[str, str]] = []
    search_path = Path(search_dir)

    # åªæœç´¢ .md æ–‡ä»¶
    for file_path in search_path.rglob("*.md"):
        try:
            content = file_path.read_text(encoding="utf-8")
            for line_num, line in enumerate(content.splitlines(), 1):
                if keyword in line:
                    matches.append(
                        {
                            "file": str(file_path),
                            "line": str(line_num),
                            "content": line.strip(),
                        }
                    )
        except Exception:
            # è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶
            continue

    return matches


def _load_continuity_index(path: Path | None = None) -> dict[str, Any]:
    target = path or Path("data/continuity/index.json")
    if not target.exists():
        raise FileNotFoundError(
            f"è¿ç»­æ€§ç´¢å¼• {target} ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ `poetry run novel-agent refresh-memory`ã€‚"
        )
    result: dict[str, Any] = json.loads(target.read_text(encoding="utf-8"))
    return result


def _get_nervus_db_path(explicit: str | None = None) -> str | None:
    return explicit or os.getenv("NERVUSDB_DB_PATH")


def _fetch_nervus_events(db_path: str) -> list[tuple[str, str]]:
    try:
        result = nervus_cli.cypher_query(
            db_path,
            """
            MATCH (ch:Chapter)-[:HAS_EVENT]->(e:Event)
            RETURN ch.id as chapter_id, e.timestamp as timestamp
            ORDER BY chapter_id, timestamp
            """,
        )
    except Exception as exc:
        raise RuntimeError(f"NervusDB æ—¶é—´çº¿æŸ¥è¯¢å¤±è´¥: {exc}") from exc

    rows = result.get("rows") if isinstance(result, dict) else result
    if not rows and isinstance(result, dict) and "result" in result:
        rows = result["result"]
    events: list[tuple[str, str]] = []
    for row in rows or []:
        chapter_id = row.get("chapter_id") or row.get("CHAPTER_ID")
        timestamp = row.get("timestamp") or row.get("TIMESTAMP")
        if chapter_id and timestamp:
            events.append((str(chapter_id), str(timestamp)))
    return events


def _fetch_nervus_references(db_path: str) -> set[tuple[str, str]]:
    try:
        result = nervus_cli.cypher_query(
            db_path,
            """
            MATCH (ch:Chapter)-[:USES_REFERENCE]->(r:Reference)
            RETURN ch.id as chapter_id, r.id as ref_id
            """,
        )
    except Exception as exc:
        raise RuntimeError(f"NervusDB å¼•ç”¨æŸ¥è¯¢å¤±è´¥: {exc}") from exc

    rows = result.get("rows") if isinstance(result, dict) else result
    if not rows and isinstance(result, dict) and "result" in result:
        rows = result["result"]
    refs: set[tuple[str, str]] = set()
    for row in rows or []:
        chapter_id = row.get("chapter_id") or row.get("CHAPTER_ID")
        ref_id = row.get("ref_id") or row.get("REF_ID")
        if chapter_id and ref_id:
            refs.add((str(chapter_id), str(ref_id)))
    return refs


def verify_strict_timeline(
    index_path: Path | None = None,
    *,
    db_path: str | None = None,
) -> dict[str, Any]:
    """æ—¶é—´çº¿ç²¾ç¡®éªŒè¯ï¼ˆå¢å¼ºç‰ˆï¼šè¾“å‡ºè¡Œå·å’Œä¿®å¤å»ºè®®ï¼‰

    Returns:
        {
            "errors": [
                {
                    "file": "chapters/ch002.md",
                    "line": 42,
                    "type": "timeline_inconsistency",
                    "message": "æ—¶é—´å€’é€€ï¼šå‰ä¸€ç« æ˜¯ 2077-03-15ï¼Œæ­¤å¤„æ˜¯ 2077-03-10",
                    "suggestion": "å°† [TIME:2077-03-10] æ”¹ä¸º [TIME:2077-03-16] æˆ–æ›´æ™š",
                    "current_value": "2077-03-10",
                    "expected_value": "2077-03-16"
                }
            ],
            "warnings": [...],
            "summary": {
                "total_errors": 2,
                "total_warnings": 1,
                "auto_fixable": True
            }
        }
    """

    data = _load_continuity_index(index_path)
    errors: list[dict[str, Any]] = []
    warnings: list[dict[str, Any]] = []

    def parse_date(value: str) -> datetime | None:
        for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%Y.%m.%d"):
            try:
                return datetime.strptime(value, fmt)
            except ValueError:
                continue
        return None

    last_date: datetime | None = None
    last_chapter_id: str = ""

    for chapter in sorted(data.get("chapters", []), key=lambda c: c.get("chapter_id", "")):
        chapter_id = chapter.get("chapter_id")
        for marker in chapter.get("time_markers", []):
            value = marker.get("value")
            line_number = marker.get("line", 0)
            dt = parse_date(value)

            if not dt:
                warnings.append(
                    {
                        "file": f"chapters/{chapter_id}.md",
                        "line": line_number,
                        "type": "unparseable_time",
                        "message": f"æ—¶é—´æ ‡è®° `{value}` æ— æ³•è§£æ",
                        "suggestion": "è¯·ä½¿ç”¨ YYYY-MM-DDã€YYYY/MM/DD æˆ– YYYY.MM.DD æ ¼å¼",
                        "current_value": value,
                    }
                )
                continue

            if last_date and dt < last_date:
                # è®¡ç®—å»ºè®®çš„æ—¥æœŸï¼ˆå‰ä¸€ä¸ªæ—¥æœŸ + 1å¤©ï¼‰
                from datetime import timedelta

                suggested_date = (last_date + timedelta(days=1)).strftime("%Y-%m-%d")

                prev_date = last_date.date()
                curr_date = dt.date()
                errors.append(
                    {
                        "file": f"chapters/{chapter_id}.md",
                        "line": line_number,
                        "type": "timeline_inconsistency",
                        "message": (
                            f"æ—¶é—´å€’é€€ï¼šå‰ä¸€ç« èŠ‚ ({last_chapter_id}) "
                            f"æ˜¯ {prev_date}ï¼Œæ­¤å¤„æ˜¯ {curr_date}"
                        ),
                        "suggestion": f"å°† [TIME:{value}] æ”¹ä¸º [TIME:{suggested_date}] æˆ–æ›´æ™š",
                        "current_value": value,
                        "expected_value": suggested_date,
                        "previous_chapter": last_chapter_id,
                        "previous_date": str(prev_date),
                    }
                )

            last_date = dt
            last_chapter_id = chapter_id

    # NervusDB æ¯”å¯¹
    nervus_db = _get_nervus_db_path(db_path)
    if nervus_db:
        try:
            db_events = _fetch_nervus_events(nervus_db)
            db_set = set(db_events)
            local_set = {
                (chapter.get("chapter_id"), marker.get("value"))
                for chapter in data.get("chapters", [])
                for marker in chapter.get("time_markers", [])
            }

            for chapter in data.get("chapters", []):
                cid = chapter.get("chapter_id")
                for marker in chapter.get("time_markers", []):
                    value = marker.get("value")
                    line_number = marker.get("line", 0)
                    if (cid, value) not in db_set:
                        errors.append(
                            {
                                "file": f"chapters/{cid}.md",
                                "line": line_number,
                                "type": "missing_in_nervus",
                                "message": f"æ—¶é—´æ ‡è®° `{value}` æœªå†™å…¥ NervusDB",
                                "suggestion": "è¿è¡Œ 'novel-agent memory ingest' åŒæ­¥åˆ° NervusDB",
                                "current_value": value,
                            }
                        )

            extra = db_set - local_set
            for cid, value in sorted(extra):
                warnings.append(
                    {
                        "file": f"chapters/{cid}.md",
                        "line": 0,
                        "type": "extra_in_nervus",
                        "message": f"NervusDB ä¸­å­˜åœ¨æœªåœ¨ç« èŠ‚å‡ºç°çš„æ—¶é—´ `{value}`",
                        "suggestion": f"æ£€æŸ¥ç« èŠ‚ {cid} æ˜¯å¦åˆ é™¤äº†æ­¤æ—¶é—´æ ‡è®°",
                        "current_value": value,
                    }
                )
        except RuntimeError as exc:
            warnings.append(
                {
                    "file": "NervusDB",
                    "line": 0,
                    "type": "nervus_error",
                    "message": str(exc),
                    "suggestion": "æ£€æŸ¥ NervusDB é…ç½®å’Œæ•°æ®åº“è¿æ¥",
                }
            )

    # è®¡ç®—è‡ªåŠ¨ä¿®å¤èƒ½åŠ›
    auto_fixable = all(err.get("expected_value") for err in errors)

    return {
        "errors": errors,
        "warnings": warnings,
        "summary": {
            "total_errors": len(errors),
            "total_warnings": len(warnings),
            "auto_fixable": auto_fixable,
        },
    }


def verify_strict_references(
    index_path: Path | None = None,
    *,
    db_path: str | None = None,
) -> dict[str, Any]:
    """å¼•ç”¨å®Œæ•´æ€§éªŒè¯ï¼ˆå¢å¼ºç‰ˆï¼šè¾“å‡ºè¡Œå·å’Œä¿®å¤å»ºè®®ï¼‰

    Returns:
        {
            "errors": [
                {
                    "file": "chapters/ch003.md",
                    "line": 56,
                    "type": "undefined_reference",
                    "message": "å¼•ç”¨ `sword_of_destiny` æœªå®šä¹‰",
                    "suggestion": "åœ¨ spec/knowledge/ ä¸­æ·»åŠ æ­¤å¼•ç”¨çš„å®šä¹‰ï¼Œæˆ–æ£€æŸ¥æ‹¼å†™é”™è¯¯",
                    "current_value": "sword_of_destiny"
                }
            ],
            "warnings": [...],
            "summary": {
                "total_errors": 1,
                "total_warnings": 2,
                "auto_fixable": False
            }
        }
    """

    data = _load_continuity_index(index_path)
    errors: list[dict[str, Any]] = []
    warnings: list[dict[str, Any]] = []

    reference_map = {ref["id"]: ref["occurrences"] for ref in data.get("references", [])}

    # æ£€æŸ¥æœªä½¿ç”¨çš„å¼•ç”¨å®šä¹‰
    for ref_id, occurrences in reference_map.items():
        if not occurrences:
            warnings.append(
                {
                    "file": "spec/knowledge/",
                    "line": 0,
                    "type": "unused_reference",
                    "message": f"å¼•ç”¨ `{ref_id}` æ— ä»»ä½•ç« èŠ‚ä½¿ç”¨",
                    "suggestion": f"è€ƒè™‘åˆ é™¤æ­¤å¼•ç”¨å®šä¹‰ï¼Œæˆ–åœ¨ç« èŠ‚ä¸­æ·»åŠ  [REF:{ref_id}]",
                    "current_value": ref_id,
                }
            )

    # æ£€æŸ¥æœªå®šä¹‰çš„å¼•ç”¨
    defined = set(reference_map.keys())
    for chapter in data.get("chapters", []):
        chapter_id = chapter.get("chapter_id")
        for ref in chapter.get("references", []):
            ref_id = ref.get("id")
            line_number = ref.get("line", 0)

            if ref_id not in defined:
                errors.append(
                    {
                        "file": f"chapters/{chapter_id}.md",
                        "line": line_number,
                        "type": "undefined_reference",
                        "message": f"å¼•ç”¨ `{ref_id}` æœªå®šä¹‰",
                        "suggestion": "åœ¨ spec/knowledge/ ä¸­æ·»åŠ æ­¤å¼•ç”¨çš„å®šä¹‰ï¼Œæˆ–æ£€æŸ¥æ‹¼å†™é”™è¯¯",
                        "current_value": ref_id,
                    }
                )

    # NervusDB æ¯”å¯¹
    nervus_db = _get_nervus_db_path(db_path)
    if nervus_db:
        try:
            db_refs = _fetch_nervus_references(nervus_db)
            db_set = set(db_refs)
            local_refs = {
                (chapter.get("chapter_id"), ref.get("id"))
                for chapter in data.get("chapters", [])
                for ref in chapter.get("references", [])
            }

            for chapter in data.get("chapters", []):
                cid = chapter.get("chapter_id")
                for ref in chapter.get("references", []):
                    ref_id = ref.get("id")
                    line_number = ref.get("line", 0)

                    if (cid, ref_id) not in db_set:
                        errors.append(
                            {
                                "file": f"chapters/{cid}.md",
                                "line": line_number,
                                "type": "missing_in_nervus",
                                "message": f"å¼•ç”¨ `{ref_id}` æœªå†™å…¥ NervusDB",
                                "suggestion": "è¿è¡Œ 'novel-agent memory ingest' åŒæ­¥åˆ° NervusDB",
                                "current_value": ref_id,
                            }
                        )

            extra_refs = db_set - local_refs
            for cid, ref_id in sorted(extra_refs):
                warnings.append(
                    {
                        "file": f"chapters/{cid}.md",
                        "line": 0,
                        "type": "extra_in_nervus",
                        "message": f"NervusDB ä¸­å­˜åœ¨æœªåœ¨ç« èŠ‚ä½¿ç”¨çš„å¼•ç”¨ `{ref_id}`",
                        "suggestion": f"æ£€æŸ¥ç« èŠ‚ {cid} æ˜¯å¦åˆ é™¤äº†æ­¤å¼•ç”¨",
                        "current_value": ref_id,
                    }
                )
        except RuntimeError as exc:
            warnings.append(
                {
                    "file": "NervusDB",
                    "line": 0,
                    "type": "nervus_error",
                    "message": str(exc),
                    "suggestion": "æ£€æŸ¥ NervusDB é…ç½®å’Œæ•°æ®åº“è¿æ¥",
                }
            )

    # å¼•ç”¨éªŒè¯é€šå¸¸ä¸èƒ½è‡ªåŠ¨ä¿®å¤ï¼ˆéœ€è¦æ‰‹åŠ¨å®šä¹‰æˆ–åˆ é™¤ï¼‰
    auto_fixable = False

    return {
        "errors": errors,
        "warnings": warnings,
        "summary": {
            "total_errors": len(errors),
            "total_warnings": len(warnings),
            "auto_fixable": auto_fixable,
        },
    }


def edit_chapter_lines(
    chapter_number: int,
    start_line: int,
    end_line: int,
    new_content: str,
    base_dir: str = "chapters",
) -> str:
    """ç²¾å‡†ä¿®æ”¹ç« èŠ‚çš„æŒ‡å®šè¡Œ

    Args:
        chapter_number: ç« èŠ‚ç¼–å·
        start_line: èµ·å§‹è¡Œå·ï¼ˆä»1å¼€å§‹ï¼‰
        end_line: ç»“æŸè¡Œå·ï¼ˆåŒ…å«ï¼Œä»1å¼€å§‹ï¼‰
        new_content: æ–°å†…å®¹ï¼ˆæ›¿æ¢æŒ‡å®šè¡Œï¼‰
        base_dir: ç« èŠ‚ç›®å½•

    Returns:
        æ“ä½œç»“æœæè¿°

    Raises:
        FileNotFoundError: ç« èŠ‚æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: è¡Œå·å‚æ•°æ— æ•ˆ

    Example:
        >>> # ä¿®æ”¹ç¬¬10-12è¡Œ
        >>> edit_chapter_lines(1, 10, 12, "æ–°çš„å†…å®¹\\næ›¿æ¢è¿™ä¸‰è¡Œ")
    """
    if chapter_number < 1:
        raise ValueError(f"ç« èŠ‚ç¼–å·å¿…é¡» >= 1ï¼Œå½“å‰: {chapter_number}")

    if start_line < 1 or end_line < start_line:
        raise ValueError(f"è¡Œå·æ— æ•ˆ: start={start_line}, end={end_line}")

    chapter_path = Path(base_dir) / f"ch{chapter_number:03d}.md"

    if not chapter_path.exists():
        raise FileNotFoundError(f"ç« èŠ‚ä¸å­˜åœ¨: {chapter_path}")

    logger.info(f"æ­£åœ¨ä¿®æ”¹ç« èŠ‚ {chapter_number} çš„ç¬¬ {start_line}-{end_line} è¡Œ: {chapter_path}")

    # è¯»å–åŸæ–‡ä»¶
    lines = chapter_path.read_text(encoding="utf-8").splitlines(keepends=True)
    total_lines = len(lines)

    if end_line > total_lines:
        raise ValueError(f"ç»“æŸè¡Œå· {end_line} è¶…å‡ºæ–‡ä»¶æ€»è¡Œæ•° {total_lines}")

    # æ›¿æ¢æŒ‡å®šè¡Œï¼ˆæ³¨æ„ï¼šè¡Œå·ä»1å¼€å§‹ï¼Œæ•°ç»„ç´¢å¼•ä»0å¼€å§‹ï¼‰
    new_lines = new_content.splitlines(keepends=True)

    # ç¡®ä¿æ–°å†…å®¹ä»¥æ¢è¡Œç¬¦ç»“å°¾
    if new_lines and not new_lines[-1].endswith("\n"):
        new_lines[-1] += "\n"

    # ç»„åˆï¼šå‰åŠéƒ¨åˆ† + æ–°å†…å®¹ + ååŠéƒ¨åˆ†
    result_lines = lines[: start_line - 1] + new_lines + lines[end_line:]

    # å†™å›æ–‡ä»¶
    chapter_path.write_text("".join(result_lines), encoding="utf-8")

    logger.info(f"âœ… æˆåŠŸä¿®æ”¹ç« èŠ‚ {chapter_number} (ç¬¬ {start_line}-{end_line} è¡Œ)")
    return (
        f"âœ… æˆåŠŸä¿®æ”¹ç« èŠ‚ {chapter_number} "
        f"(ç¬¬ {start_line}-{end_line} è¡Œï¼Œå…± {len(new_lines)} è¡Œæ–°å†…å®¹)"
    )


def replace_in_file(
    file_path: str,
    search_text: str,
    replacement: str,
    occurrence: int | None = None,
) -> str:
    """åœ¨æ–‡ä»¶ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢æ–‡æœ¬

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        search_text: è¦æŸ¥æ‰¾çš„æ–‡æœ¬
        replacement: æ›¿æ¢æ–‡æœ¬
        occurrence: æ›¿æ¢ç¬¬å‡ æ¬¡å‡ºç°ï¼ˆNone=å…¨éƒ¨æ›¿æ¢ï¼Œ1=ç¬¬ä¸€æ¬¡ï¼Œ2=ç¬¬äºŒæ¬¡...ï¼‰

    Returns:
        æ“ä½œç»“æœæè¿°

    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: search_text ä¸å­˜åœ¨

    Example:
        >>> # æ›¿æ¢æ‰€æœ‰"å¼ ä¸‰"ä¸º"æå››"
        >>> replace_in_file("chapters/ch001.md", "å¼ ä¸‰", "æå››")

        >>> # åªæ›¿æ¢ç¬¬ä¸€æ¬¡å‡ºç°çš„"å¼ ä¸‰"
        >>> replace_in_file("chapters/ch001.md", "å¼ ä¸‰", "æå››", occurrence=1)
    """
    path = Path(file_path)

    if not path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    logger.info(f"æ­£åœ¨æŸ¥æ‰¾æ›¿æ¢: {file_path} ('{search_text}' â†’ '{replacement}')")

    content = path.read_text(encoding="utf-8")

    if search_text not in content:
        raise ValueError(f"æœªæ‰¾åˆ°è¦æ›¿æ¢çš„æ–‡æœ¬: '{search_text}'")

    # è®¡ç®—å‡ºç°æ¬¡æ•°
    count = content.count(search_text)

    if occurrence is not None:
        if occurrence < 1 or occurrence > count:
            raise ValueError(f"occurrence å‚æ•°æ— æ•ˆ: {occurrence} (æ–‡æœ¬å…±å‡ºç° {count} æ¬¡)")

        # æ›¿æ¢æŒ‡å®šçš„ç¬¬ N æ¬¡å‡ºç°
        parts = content.split(search_text)
        new_content = (
            search_text.join(parts[:occurrence])
            + replacement
            + search_text.join(parts[occurrence:])
        )
        replaced_count = 1
    else:
        # æ›¿æ¢æ‰€æœ‰å‡ºç°
        new_content = content.replace(search_text, replacement)
        replaced_count = count

    path.write_text(new_content, encoding="utf-8")

    logger.info(f"âœ… æˆåŠŸæ›¿æ¢ {replaced_count} å¤„æ–‡æœ¬: {file_path}")
    return f"âœ… æˆåŠŸæ›¿æ¢ {replaced_count} å¤„æ–‡æœ¬: {file_path} (å…±å‡ºç° {count} æ¬¡)"


def multi_edit(operations: list[dict[str, Any]]) -> str:
    """æ‰¹é‡ç¼–è¾‘å¤šä¸ªæ–‡ä»¶

    Args:
        operations: ç¼–è¾‘æ“ä½œåˆ—è¡¨ï¼Œæ¯ä¸ªæ“ä½œåŒ…å«ï¼š
            - type: "replace" | "edit_lines"
            - file: æ–‡ä»¶è·¯å¾„
            - ... å…¶ä»–å‚æ•°å–å†³äº type

    Returns:
        æ“ä½œç»“æœæè¿°

    Raises:
        ValueError: æ“ä½œå‚æ•°æ— æ•ˆ
        RuntimeError: ç¼–è¾‘å¤±è´¥ï¼ˆä¼šå›æ»šæ‰€æœ‰æ“ä½œï¼‰

    Example:
        >>> operations = [
        ...     {
        ...         "type": "replace",
        ...         "file": "chapters/ch001.md",
        ...         "search": "å¼ ä¸‰",
        ...         "replace": "æå››"
        ...     },
        ...     {
        ...         "type": "replace",
        ...         "file": "chapters/ch002.md",
        ...         "search": "å¼ ä¸‰",
        ...         "replace": "æå››"
        ...     },
        ... ]
        >>> multi_edit(operations)
    """
    if not operations:
        return "âš ï¸ æ²¡æœ‰æ“ä½œéœ€è¦æ‰§è¡Œ"

    logger.info(f"å¼€å§‹æ‰¹é‡ç¼–è¾‘ï¼š{len(operations)} ä¸ªæ“ä½œ")

    # å¤‡ä»½æ‰€æœ‰æ–‡ä»¶
    backups: dict[str, str] = {}
    modified_files: list[str] = []

    try:
        # ç¬¬ä¸€æ­¥ï¼šå¤‡ä»½æ‰€æœ‰æ–‡ä»¶
        for op in operations:
            file_path = op.get("file")
            if not file_path:
                raise ValueError(f"æ“ä½œç¼ºå°‘ 'file' å‚æ•°: {op}")

            path = Path(file_path)
            if path.exists() and file_path not in backups:
                backups[file_path] = path.read_text(encoding="utf-8")

        # ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œæ‰€æœ‰æ“ä½œ
        for i, op in enumerate(operations, 1):
            op_type = op.get("type")
            file_path = op["file"]

            logger.debug(f"æ‰§è¡Œæ“ä½œ {i}/{len(operations)}: {op_type} on {file_path}")

            if op_type == "replace":
                replace_in_file(
                    file_path,
                    op["search"],
                    op["replace"],
                    op.get("occurrence"),
                )
                modified_files.append(file_path)

            elif op_type == "edit_lines":
                # æå–ç« èŠ‚ç¼–å·
                chapter_number = op.get("chapter_number")
                if chapter_number is None:
                    # å°è¯•ä»æ–‡ä»¶åæå–
                    match = Path(file_path).stem
                    if match.startswith("ch") and match[2:5].isdigit():
                        chapter_number = int(match[2:5])
                    else:
                        raise ValueError(f"æ— æ³•ç¡®å®šç« èŠ‚ç¼–å·: {file_path}")

                edit_chapter_lines(
                    chapter_number,
                    op["start_line"],
                    op["end_line"],
                    op["new_content"],
                    base_dir=str(Path(file_path).parent),
                )
                modified_files.append(file_path)

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {op_type}")

        logger.info(f"âœ… æ‰¹é‡ç¼–è¾‘å®Œæˆï¼šä¿®æ”¹äº† {len(set(modified_files))} ä¸ªæ–‡ä»¶")
        return (
            f"âœ… æ‰¹é‡ç¼–è¾‘å®Œæˆï¼šä¿®æ”¹äº† {len(set(modified_files))} ä¸ªæ–‡ä»¶ ({len(operations)} ä¸ªæ“ä½œ)"
        )

    except Exception as e:
        # å›æ»šæ‰€æœ‰ä¿®æ”¹
        logger.error(f"æ‰¹é‡ç¼–è¾‘å¤±è´¥ï¼Œæ­£åœ¨å›æ»š: {e}")

        for file_path, backup_content in backups.items():
            try:
                Path(file_path).write_text(backup_content, encoding="utf-8")
                logger.debug(f"å·²å›æ»š: {file_path}")
            except Exception as rollback_err:
                logger.error(f"å›æ»šå¤±è´¥: {file_path} - {rollback_err}")

        raise RuntimeError(f"æ‰¹é‡ç¼–è¾‘å¤±è´¥ï¼Œå·²å›æ»šæ‰€æœ‰ä¿®æ”¹: {e}") from e


# ========== å›¾æŸ¥è¯¢å·¥å…· (Graph Query Tools) ==========


def smart_context_search_tool(
    query: str,
    search_type: str = "all",
    max_hops: int = 2,
    limit: int = 10,
) -> str:
    """æ™ºèƒ½ä¸Šä¸‹æ–‡æœç´¢ï¼ˆåŸºäºå›¾æ•°æ®åº“ï¼‰

    ä½¿ç”¨ NervusDB å›¾æ•°æ®åº“è¿›è¡Œæ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œæ¯”å‘é‡æ£€ç´¢æ›´ç²¾å‡†ã€æ›´å¯è§£é‡Šã€‚

    Args:
        query: æœç´¢æŸ¥è¯¢ï¼ˆå¦‚"å¼ ä¸‰å’Œæå››çš„å…³ç³»"ï¼‰
        search_type: 'character' | 'location' | 'event' | 'foreshadow' | 'all'
        max_hops: æœ€å¤§å…³ç³»è·³æ•°ï¼ˆ1-3ï¼Œé»˜è®¤ 2ï¼‰
        limit: æœ€å¤šè¿”å›ç»“æœæ•°ï¼ˆé»˜è®¤ 10ï¼‰

    Returns:
        æ ¼å¼åŒ–çš„æœç´¢ç»“æœï¼ŒåŒ…å«ï¼š
        - ç›´æ¥åŒ¹é…çš„å®ä½“
        - é€šè¿‡å…³ç³»å…³è”çš„å®ä½“
        - å›¾è·¯å¾„å’Œç½®ä¿¡åº¦
        - ç»Ÿè®¡ä¿¡æ¯

    Example:
        >>> smart_context_search_tool("å¼ ä¸‰", "character", max_hops=2, limit=5)
        æ‰¾åˆ° 5 ä¸ªç›¸å…³ç»“æœï¼š
          - character: 2 ä¸ª
          - chapter: 3 ä¸ª

        1. [ç›´æ¥åŒ¹é…] å¼ ä¸‰ (character)
           ç½®ä¿¡åº¦: 1.0

        2. [1 è·³å…³ç³»] æå›› (character)
           è·¯å¾„: å¼ ä¸‰ -> æå››
           ç½®ä¿¡åº¦: 0.5

        ...
    """
    from .graph_query import smart_context_search

    logger.info(f"å›¾æŸ¥è¯¢: {query}, ç±»å‹={search_type}, è·³æ•°={max_hops}")

    try:
        db_path = os.getenv("NOVEL_GRAPH_DB", "data/novel-graph.nervusdb")
        result = smart_context_search(
            query=query,
            db_path=db_path,
            search_type=search_type,  # type: ignore
            max_hops=max_hops,
            limit=limit,
        )

        # æ ¼å¼åŒ–è¾“å‡º
        output = [result["summary"], ""]

        for i, item in enumerate(result["results"], 1):
            output.append(f"{i}. [{item['relevance']}] {item['name']} ({item['type']})")
            if item["path"] and len(item["path"]) > 1:
                output.append(f"   è·¯å¾„: {' -> '.join(item['path'])}")
            output.append(f"   ç½®ä¿¡åº¦: {item['confidence']:.2f}")
            output.append("")

        # æ·»åŠ ç»Ÿè®¡
        stats = result["graph_stats"]
        output.append(
            f"ğŸ“Š ç»Ÿè®¡: æœç´¢äº† {stats['nodes_searched']} ä¸ªèŠ‚ç‚¹ï¼Œæœ€å¤§æ·±åº¦ {stats['max_depth']}"
        )

        return "\n".join(output)

    except Exception as e:
        logger.error(f"å›¾æŸ¥è¯¢å¤±è´¥: {e}")
        return f"âŒ å›¾æŸ¥è¯¢å¤±è´¥: {e}\næç¤ºï¼šè¯·å…ˆè¿è¡Œ 'novel-agent build-graph' æ„å»ºå›¾æ•°æ®åº“"


def build_character_network_tool(character_names: str | None = None) -> str:
    """æ„å»ºè§’è‰²å…³ç³»ç½‘ç»œå›¾

    åˆ†æè§’è‰²ä¹‹é—´çš„å…³ç³»ï¼Œæ„å»ºç¤¾äº¤ç½‘ç»œå›¾ã€‚

    Args:
        character_names: è§’è‰²ååˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚"å¼ ä¸‰,æå››,ç‹äº”"ï¼‰
                        ç•™ç©ºåˆ™åˆ†ææ‰€æœ‰è§’è‰²

    Returns:
        æ ¼å¼åŒ–çš„ç½‘ç»œä¿¡æ¯ï¼š
        - èŠ‚ç‚¹ï¼ˆè§’è‰²ï¼‰åˆ—è¡¨
        - è¾¹ï¼ˆå…³ç³»ï¼‰åˆ—è¡¨
        - ç¤¾åŒºï¼ˆç¾¤ç»„ï¼‰æ£€æµ‹ç»“æœ

    Example:
        >>> build_character_network_tool("å¼ ä¸‰,æå››")
        è§’è‰²ç½‘ç»œåˆ†æç»“æœï¼š

        èŠ‚ç‚¹ (2 ä¸ª):
        1. å¼ ä¸‰ (protagonist)
        2. æå›› (supporting)

        å…³ç³» (1 æ¡):
        1. å¼ ä¸‰ -[knows]-> æå›› (å¼ºåº¦: 0.9)

        ç¤¾åŒº (1 ä¸ª):
        - ç¤¾åŒº 1: å¼ ä¸‰, æå›› (2 äºº)
    """
    from .graph_query import build_character_network

    logger.info(f"æ„å»ºè§’è‰²ç½‘ç»œ: {character_names or 'æ‰€æœ‰è§’è‰²'}")

    try:
        db_path = os.getenv("NOVEL_GRAPH_DB", "data/novel-graph.nervusdb")
        names_list = (
            [n.strip() for n in character_names.split(",") if n.strip()]
            if character_names
            else None
        )

        result = build_character_network(db_path=db_path, character_names=names_list)

        # æ ¼å¼åŒ–è¾“å‡º
        output = ["è§’è‰²ç½‘ç»œåˆ†æç»“æœï¼š", ""]

        # èŠ‚ç‚¹
        output.append(f"èŠ‚ç‚¹ ({len(result['nodes'])} ä¸ª):")
        for i, node in enumerate(result["nodes"][:20], 1):  # é™åˆ¶æ˜¾ç¤ºå‰ 20 ä¸ª
            node_type = node.get("properties", {}).get("type", node["type"])
            output.append(f"{i}. {node['label']} ({node_type})")
        if len(result["nodes"]) > 20:
            output.append(f"... è¿˜æœ‰ {len(result['nodes']) - 20} ä¸ªèŠ‚ç‚¹")
        output.append("")

        # å…³ç³»
        output.append(f"å…³ç³» ({len(result['edges'])} æ¡):")
        for i, edge in enumerate(result["edges"][:20], 1):
            weight = edge.get("weight", 1.0)
            relation = f"{edge['source']} -[{edge['relation']}]-> {edge['target']}"
            output.append(f"{i}. {relation} (å¼ºåº¦: {weight:.2f})")
        if len(result["edges"]) > 20:
            output.append(f"... è¿˜æœ‰ {len(result['edges']) - 20} æ¡å…³ç³»")
        output.append("")

        # ç¤¾åŒº
        output.append(f"ç¤¾åŒº ({len(result['clusters'])} ä¸ª):")
        for cluster in result["clusters"][:10]:
            members_str = ", ".join(cluster["members"][:5])
            if len(cluster["members"]) > 5:
                members_str += f" ... å…± {cluster['size']} äºº"
            output.append(f"- {cluster['label']}: {members_str}")
        if len(result["clusters"]) > 10:
            output.append(f"... è¿˜æœ‰ {len(result['clusters']) - 10} ä¸ªç¤¾åŒº")

        return "\n".join(output)

    except Exception as e:
        logger.error(f"æ„å»ºè§’è‰²ç½‘ç»œå¤±è´¥: {e}")
        return f"âŒ æ„å»ºè§’è‰²ç½‘ç»œå¤±è´¥: {e}\næç¤ºï¼šè¯·å…ˆè¿è¡Œ 'novel-agent build-graph' æ„å»ºå›¾æ•°æ®åº“"


def trace_foreshadow_tool(foreshadow_id: str) -> str:
    """è¿½æº¯ä¼ç¬”å®Œæ•´é“¾æ¡

    è¿½è¸ªä¼ç¬”ä»åŸ‹ä¸‹åˆ°æ­æ™“çš„å®Œæ•´è¿‡ç¨‹ã€‚

    Args:
        foreshadow_id: ä¼ç¬” IDï¼ˆå¦‚ "foreshadow_001"ï¼‰

    Returns:
        æ ¼å¼åŒ–çš„ä¼ç¬”è¿½æº¯ç»“æœï¼š
        - Setupï¼ˆåŸ‹ç¬”ï¼‰ç« èŠ‚
        - Hintsï¼ˆæš—ç¤ºï¼‰åˆ—è¡¨
        - Revealï¼ˆæ­æ™“ï¼‰ç« èŠ‚
        - çŠ¶æ€ï¼ˆå·²è§£å†³/æœªè§£å†³ï¼‰

    Example:
        >>> trace_foreshadow_tool("foreshadow_001")
        ä¼ç¬”è¿½æº¯: foreshadow_001

        ğŸ“ åŸ‹ç¬” (Setup):
        - ç¬¬ 5 ç« 

        ğŸ’¡ æš—ç¤º (Hints):
        - ç¬¬ 5 ç« : é¦–æ¬¡æåŠ
        - ç¬¬ 8 ç« : éšæ™¦æš—ç¤º
        - ç¬¬ 12 ç« : æ˜ç¡®æš—ç¤º

        ğŸ¯ æ­æ™“ (Reveal):
        - ç¬¬ 20 ç« 

        âœ… çŠ¶æ€: å·²è§£å†³
    """
    from .graph_query import trace_foreshadow

    logger.info(f"è¿½æº¯ä¼ç¬”: {foreshadow_id}")

    try:
        db_path = os.getenv("NOVEL_GRAPH_DB", "data/novel-graph.nervusdb")
        result = trace_foreshadow(foreshadow_id=foreshadow_id, db_path=db_path)

        if "error" in result:
            return f"âŒ {result['error']}"

        # æ ¼å¼åŒ–è¾“å‡º
        output = [f"ä¼ç¬”è¿½æº¯: {foreshadow_id}", ""]

        # Setup
        if result.get("setup"):
            setup = result["setup"]
            output.append("ğŸ“ åŸ‹ç¬” (Setup):")
            output.append(f"- ç¬¬ {setup['chapter']} ç« ")
            output.append("")

        # Hints
        hints = result.get("hints", [])
        if hints:
            output.append(f"ğŸ’¡ æš—ç¤º (Hints, {len(hints)} å¤„):")
            for hint in hints:
                output.append(f"- ç¬¬ {hint['chapter']} ç« ")
            output.append("")

        # Reveal
        if result.get("reveal"):
            reveal = result["reveal"]
            output.append("ğŸ¯ æ­æ™“ (Reveal):")
            output.append(f"- ç¬¬ {reveal['chapter']} ç« ")
            output.append("")

        # Status
        status_emoji = "âœ…" if result["status"] == "resolved" else "âš ï¸ "
        status_text = "å·²è§£å†³" if result["status"] == "resolved" else "æœªè§£å†³"
        output.append(f"{status_emoji} çŠ¶æ€: {status_text}")

        return "\n".join(output)

    except Exception as e:
        logger.error(f"è¿½æº¯ä¼ç¬”å¤±è´¥: {e}")
        return f"âŒ è¿½æº¯ä¼ç¬”å¤±è´¥: {e}\næç¤ºï¼šè¯·å…ˆè¿è¡Œ 'novel-agent build-graph' æ„å»ºå›¾æ•°æ®åº“"


def read_multiple_files(paths: str) -> str:
    """æ‰¹é‡è¯»å–å¤šä¸ªæ–‡ä»¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰

    ä¸€æ¬¡æ€§è¯»å–å¤šä¸ªæ–‡ä»¶ï¼Œå‡å°‘ API è°ƒç”¨æ¬¡æ•°

    Args:
        paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¦‚ "ch1.md,ch2.md,ch3.md"ï¼‰

    Returns:
        æ‰€æœ‰æ–‡ä»¶çš„å†…å®¹ï¼Œæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²

    Raises:
        FileNotFoundError: æŸä¸ªæ–‡ä»¶ä¸å­˜åœ¨
    """
    path_list = [p.strip() for p in paths.split(",")]
    logger.info(f"æ‰¹é‡è¯»å– {len(path_list)} ä¸ªæ–‡ä»¶")

    results = []
    for path in path_list:
        try:
            content = read_file(path)
            results.append(f"=== {path} ===\n{content}\n")
        except FileNotFoundError:
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {path}")
            results.append(f"=== {path} ===\nâŒ æ–‡ä»¶ä¸å­˜åœ¨\n")

    return "\n".join(results)


# å·¥å…·è£…é¥°å™¨åŒ…è£…ï¼ˆç”¨äº LangChainï¼‰
read_multiple_files_tool = lc_tool(read_multiple_files)
